\documentclass{article}

\usepackage{mlsys2025}
\usepackage{natbib}
\usepackage{url}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{placeins}
\usepackage{listings}

% Listings setup for prompt blocks (two-column friendly, no boxes)
\lstset{
  basicstyle=\ttfamily\scriptsize,
  breaklines=true,
  breakatwhitespace=false, % allow breaking long tokens
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  tabsize=2,
  frame=none,
  xleftmargin=0pt,
  xrightmargin=0pt,
  aboveskip=0.5em,
  belowskip=0.5em,
}


\title{Moral Susceptibility and Robustness\\ in Large Language Models}




% For double‑blind submission, leave author info empty under the MLSys style.
\author{Davi Bastos Costa, Felippe Alves \& Renato Vicente \\
TELUS Digital Research Hub\\ 
Center for Artificial Intelligence and Machine Learning\\
Institute of Mathematics, Statistics and Computer Science\\
University of São Paulo \\
\texttt{\{davi.costa,falves,rvicente\}@usp.br} \\
}

\begin{document}

\maketitle

\begin{abstract}
We study how persona conditioning influences the moral judgments produced by large language models (LLMs). Using the 30-item Moral Foundations Questionnaire (MFQ-30), we elicit repeated ratings across diverse personas and models, and introduce a benchmark that quantifies two properties: (i) moral susceptibility (the extent to which MFQ subscale scores shift under different personas), and (ii) robustness (the stability of ratings under repeated sampling and persona variation). We describe a simple, reproducible experimental protocol and propose variance- and effect-size-based metrics alongside mixed-effects analyses to isolate persona-related variance components. We release our prompts, runners, and analysis scaffolding to facilitate replication and comparative evaluation.
\end{abstract}

\section{Introduction}
Reliable benchmarks for the social capabilities of large language models (LLMs) are increasingly important as these systems are deployed in interactive, multi-agent settings where outcomes hinge on social intelligence and strategic reasoning. Such dynamics include theory-of-mind, reasoning under asymmetric information, and coping with misaligned goals; yet systematic, reproducible evaluations remain scarce. Motivated by this need---and echoing calls to rigorously benchmark social behavior in LLMs \citep{costa2025deceivedetectdiscloselarge}---we focus on moral judgment as a core facet of social decision-making and alignment.



This paper introduces a benchmark based on the Moral Foundations Questionnaire (MFQ-30), a widely used instrument in moral psychology that measures five moral foundations: Harm/Care, Fairness/Reciprocity, In-group/Loyalty, Authority/Respect, and Purity/Sanctity \citep{graham2009liberals,haidt2007when}. We operationalize moral susceptibility as the variation in MFQ subscale scores across personas, and robustness as the stability of ratings across repeated trials and persona perturbations. Our contributions are:
\begin{enumerate}
  \item A standardized, open protocol for eliciting MFQ-30 ratings from LLMs under persona conditioning, including prompts and a lightweight runner.
  \item A set of susceptibility and robustness metrics grounded in variance components, effect sizes, and reliability analysis.
  \item An empirical study across multiple models and personas, with guidance for statistical analysis and reporting.
\end{enumerate}

Recent MFQ-based studies profile LLM value orientations and alignment. \citet{abdulhai-etal-2024-moral} adapt MFQ prompts to derive foundation scores, compare them to human surveys, and show that targeted prompts can shift profiles and affect downstream donations. \citet{nunes2024hypocrites} combine MFQ with MFV to reveal inconsistencies between abstract and concrete judgments. \citet{aksoy2024whose} use MFQ-2 across eight languages to expose cultural/linguistic variability, and \citet{bajpai2024insights} compare MFQ-20 and moral competence between humans and chatbots, finding LLMs emphasize individualist foundations and lag human competence. In parallel, MoralBench \citep{ji2025moralbenchmoralevaluationllms} offers a broad task suite; our MFQ persona framework complements it by isolating persona-driven shifts relative to a self baseline.

\section{Moral Robustness and Susceptibility Benchmark}
We define a benchmark to evaluate two complementary dimensions of persona sensitivity in LLMs.

\paragraph{Robustness} The stability of MFQ ratings under repeated sampling and small persona perturbations (e.g., paraphrases). Operationally, we report a simple index defined as the inverse of the average per-item standard deviation across repetitions (higher is more stable).

\paragraph{Moral susceptibility} The degree to which MFQ subscale scores shift as persona descriptions change. High susceptibility indicates strong persona-driven modulation of moral judgments; low susceptibility indicates persona-invariant responses.

\subsection{MFQ}
The MFQ-30 comprises 30 items split into two sections: 15 relevance judgments (how relevant specific considerations are when deciding right from wrong) and 15 agreement statements (level of agreement with moral propositions) \citep{graham2011mfq}. Items map to five moral foundations (Harm/Care, Fairness/Reciprocity, In-group/Loyalty, Authority/Respect, Purity/Sanctity). Following common practice, filler items (e.g., canonical item indices 6 and 22 in some MFQ-30 versions) are excluded from subscale scoring. Subscale scores are computed by averaging the items associated with each foundation within each section and then combining sections (e.g., mean of relevance and agreement for that foundation), or by an alternative pre-registered scheme.

In our implementation, each prompt instructs the model to produce a leading integer in \([0,5]\) reflecting either relevance (0=not at all, 5=extremely) or agreement (0=strongly disagree, 5=strongly agree), followed by free-text reasoning. Ratings are parsed by extracting the first digit \([0,5]\) from the response. Figure~\ref{fig:mfq-profiles} illustrates the resulting MFQ relevance profile across models using the self (no-persona) baseline.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../results/moral_foundations_relevance_profiles.png}
  \caption{Moral foundation relevance profiles (self/no-persona baseline). Points show mean relevance per foundation; error bars denote standard errors across items within each foundation.}
  \label{fig:mfq-profiles}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../results/persona_moral_foundations_relevance_profiles.png}
  \caption{Moral foundation relevance profiles for five randomly selected personas, averaged across models. This visualization highlights an averaged effect of persona identity (persona\_id) on MFQ relevance patterns. See the Personas appendix for descriptions; indices match the zero-based persona\_id used in our runs.}
  \label{fig:persona-mfq-profiles}
\end{figure}

\subsection{Experimental Methodology}
We use a simple, reproducible runner that iterates through MFQ-30 items for a list of personas and repeats each item multiple times to characterize response variability. The runner supports local GGUF models as well as API-hosted models through a uniform interface. Concretely:
\begin{itemize}
  \item \textbf{Personas:} A JSON file provides persona descriptions (plain text). By default, each persona is used as-is and identified by its index.
  \item \textbf{Prompting:} For each persona and item, the model receives a roleplaying instruction ("You are roleplaying as the following persona: \ldots") plus the MFQ item prompt. The prompt requests a leading integer rating in \([0,5]\) and then reasoning.
  \item \textbf{Repetitions:} Each persona--question pair is queried \(n\) times (default \(n=10\)) to estimate within-persona variability and enable reliability analysis.
  \item \textbf{Decoding:} We use low temperature (default 0.1) and a small \texttt{max\_tokens} (default 5) to elicit short, rating-first outputs. Ratings are parsed with a conservative regex; failures are recorded as \(-1\).
  \item \textbf{Logging:} Each response is streamed to CSV with fields: persona\_id, question\_id, run\_index, rating, truncated response text, and timestamp.
  \item \textbf{Models:} We include local chat-tuned GGUF models (e.g., Mistral, Llama, Qwen) and hosted models (e.g., Anthropic, OpenAI) when API keys are configured.
\end{itemize}


\subsection{Statistical Analysis}



This section formalizes the quantities we compute from the MFQ runs and how we summarize them into moral robustness susceptibility metrics with uncertainty.

Let \(\mathcal{P}\) be the set of personas, \(\mathcal{Q}\) the set of 30 scored MFQ items, and \(R\) the number of repeated queries per persona--item pair. For persona \(p\), item \(q\), and repetition \(i=1,\ldots,R\), let \(y_{pqi}\in\{0,\ldots,5\}\) be the parsed rating.

For each persona--item pair we compute the sample mean and the standard deviation across repetitions
\begin{align}
  \bar{y}_{pq} &= \frac{1}{R} \sum_{i=1}^{R} y_{pqi}, \label{eq:persona-question-mean}\\
  u_{pq} &= \sqrt{\frac{1}{R-1} \sum_{i=1}^{R} \big(y_{pqi} - \bar{y}_{pq}\big)^2}, \label{eq:persona-question-se}
\end{align}
so that \(u_{pq}\) is the standard deviation (SD) across repetitions.

\paragraph{Susceptibility (between-persona sensitivity)} To stabilize estimates across many personas, we partition \(\mathcal{P}\) into \(G\) disjoint groups \(\mathcal{P}_1,\ldots,\mathcal{P}_G\) of equal size (default 10 personas per group). For each item \(q\) and group \(g\), we compute the sample standard deviation of persona means
\begin{align}
  & s_{qg} = \sqrt{\frac{1}{|\mathcal{P}_g|-1} \sum_{p \in \mathcal{P}_g} \Big(\bar{y}_{pq} - \bar{y}_{gq}\Big)^2},\\
  & \bar{y}_{gq} = \frac{1}{|\mathcal{P}_g|} \sum_{p \in \mathcal{P}_g} \bar{y}_{pq},
  \label{eq:question-dispersion}
\end{align}
and average across items to obtain a group-level susceptibility sample
\begin{equation}
  S_g = \frac{1}{|\mathcal{Q}|} \sum_{q \in \mathcal{Q}} s_{qg}.\label{eq:group-susceptibility}
\end{equation}
The reported susceptibility is the mean over groups
\begin{equation}
  S = \frac{1}{G} \sum_{g=1}^{G} S_g,\label{eq:overall-susceptibility}
\end{equation}
with its standard error estimated from the between-group variability
\begin{equation}
  \sigma_S = \frac{\sqrt{\frac{1}{G-1} \sum_{g=1}^{G} (S_g - S)^2}}{\sqrt{G}}.\label{eq:susceptibility-se}
\end{equation}
Foundation-specific susceptibilities reuse \eqref{eq:question-dispersion}--\eqref{eq:susceptibility-se} after restricting \(\mathcal{Q}\) to the item subset \(\mathcal{Q}_f\) for foundation \(f\).

\paragraph{Robustness (trial-level stability)} We summarize within-pair variability by averaging the SDs in \eqref{eq:persona-question-se} over personas and items
\begin{equation}
  \bar{u} = \frac{1}{|\mathcal{P}|\,|\mathcal{Q}|} \sum_{p \in \mathcal{P}} \sum_{q \in \mathcal{Q}} u_{pq}.\label{eq:mean-uncertainty}
\end{equation}
Our robustness index is the reciprocal
\begin{equation}
  R = \frac{1}{\bar{u}}.\label{eq:robustness}
\end{equation}
Let the (sample) standard deviation of the \(u_{pq}\) values be
\begin{equation}
  s_u = \sqrt{\frac{1}{|\mathcal{P}|\,|\mathcal{Q}| - 1} \sum_{p \in \mathcal{P}} \sum_{q \in \mathcal{Q}} \Big(u_{pq} - \bar{u}\Big)^2}.\label{eq:uncertainty-sd}
\end{equation}
Then the SE of \(\bar{u}\) is \(\sigma_{\bar{u}} = s_u / \sqrt{|\mathcal{P}|\,|\mathcal{Q}|}\). Applying the delta method to \eqref{eq:robustness} yields the propagated SE for robustness
\begin{equation}
  \sigma_R = \frac{\sigma_{\bar{u}}}{\bar{u}^2}.\label{eq:robustness-se}
\end{equation}
Foundation-level robustness repeats \eqref{eq:mean-uncertainty}--\eqref{eq:robustness-se} with sums over \(\mathcal{Q}_f\).

\paragraph{Cross-model normalization} The z-score table (Table~\ref{tab:summary_by_model_with_z}) summarizes relative performance across models. For metric \(M \in \{S,R\}\) and foundation \(f\), let \(V_{mf}^{(M)}\) be model \(m\)'s estimate with SE \(\sigma_{V, mf}^{(M)}\). With across-model mean \(\mu_f^{(M)}\) and SD \(\sigma_f^{(M)}\), the z-score is
\begin{equation}
  Z_{mf}^{(M)} = \frac{V_{mf}^{(M)} - \mu_f^{(M)}}{\sigma_f^{(M)}},\qquad
  \sigma_{Z, mf}^{(M)} = \frac{\sigma_{V, mf}^{(M)}}{\sigma_f^{(M)}}.\label{eq:zscore}
\end{equation}



\section{Results}

We present robustness and susceptibility by model and foundation, plus a z-score summary table across models.

% Merged summary + z-score table across models
\input{table_summary_by_model_with_z}

\subsection{Robustness}
We quantify trial-level stability by first computing the sample standard deviation across repetitions for each persona–question pair (Eq.~\ref{eq:persona-question-se}), averaging these to obtain \(\bar{u}\) (Eq.~\ref{eq:mean-uncertainty}), and defining robustness as \(R = 1/\bar{u}\) (Eq.~\ref{eq:robustness}) with uncertainty propagated via Eq.~\ref{eq:robustness-se}.

% Robustness (overall + five foundations)
\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.3\linewidth]{../results/robustness.png}\hfill
  \includegraphics[width=0.3\linewidth]{../results/robustness_harm_care.png}\hfill
  \includegraphics[width=0.3\linewidth]{../results/robustness_fairness_reciprocity.png}\\[0.75em]
  \includegraphics[width=0.3\linewidth]{../results/robustness_in_group_loyalty.png}\hfill
  \includegraphics[width=0.3\linewidth]{../results/robustness_authority_respect.png}\hfill
  \includegraphics[width=0.3\linewidth]{../results/robustness_purity_sanctity.png}
  \caption{Six-panel summary of robustness (inverse of average per-item standard deviation across repetitions). Top row: overall benchmark, Harm/Care, and Fairness/Reciprocity. Bottom row: In-group/Loyalty, Authority/Respect, and Purity/Sanctity. Error bars show propagated SE via delta method; higher values indicate greater rating stability.}
  \label{fig:robustness}
\end{figure*}

\noindent\textit{Observations.} Robustness exhibits clear within-family structure across models. We observe a strong correlation by model family (e.g., families cluster together across foundations), with the Claude family consistently the most robust; notably, \emph{Claude Sonnet} outperforms all others by a sizeable margin across foundations. In contrast, the Grok models are the least robust on average. We do not find a meaningful dependence on model size or SKU within families: the GPT-4.1 variants (normal, mini, nano) perform similarly, and Grok-4 versus Grok-4-fast shows no systematic size effect on robustness. These trends are visible in Figure~\ref{fig:robustness} and summarized in the z-score table (Table~\ref{tab:summary_by_model_with_z}).

\subsection{Susceptibility}
We assess between-persona sensitivity by computing within-group dispersion of persona means per item (Eq.~\ref{eq:question-dispersion}), averaging across items to form group-level samples (Eq.~\ref{eq:group-susceptibility}), and reporting the across-group mean and its SE (Eqs.~\ref{eq:overall-susceptibility}--\ref{eq:susceptibility-se}).

% Susceptibility (overall + five foundations)
\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.3\linewidth]{../results/susceptibility.png}\hfill
  \includegraphics[width=0.3\linewidth]{../results/susceptibility_harm_care.png}\hfill
  \includegraphics[width=0.3\linewidth]{../results/susceptibility_fairness_reciprocity.png}\\[0.75em]
  \includegraphics[width=0.3\linewidth]{../results/susceptibility_in_group_loyalty.png}\hfill
  \includegraphics[width=0.3\linewidth]{../results/susceptibility_authority_respect.png}\hfill
  \includegraphics[width=0.3\linewidth]{../results/susceptibility_purity_sanctity.png}
  \caption{Six-panel summary of moral susceptibility (mean $\pm$ SE across persona groups). Top row: overall benchmark, Harm/Care, and Fairness/Reciprocity. Bottom row: In-group/Loyalty, Authority/Respect, and Purity/Sanctity. Higher values indicate larger persona-driven shifts in MFQ subscale scores.}
  \label{fig:susceptibility}
\end{figure*}

\noindent\textit{Observations.} Susceptibility is more idiosyncratic: we do not observe strong correlation within model families, and rankings vary across foundations. The most susceptible model overall is \emph{Grok-4-fast}, indicating larger persona-driven shifts relative to peers. Across GPT-4.1 variants there is no consistent size pattern (normal, mini, nano are comparable). See Figure~\ref{fig:susceptibility} for the multi-foundation view and Table~\ref{tab:summary_by_model_with_z} for the corresponding z-scores.



\paragraph{Qualitative analysis}
Provide representative excerpts of reasoning (with personas anonymized) that illustrate high-susceptibility shifts versus robustly stable judgments.

\section{Conclusion}
We propose a principled benchmark for quantifying persona-driven shifts in LLM moral judgments using the MFQ-30. Our framework separates susceptibility (persona sensitivity) and robustness (rating stability), supports multiple model classes, and relies on transparent, easily repeatable procedures. Future work includes expanding persona taxonomies, stress-testing prompt formats, modeling reasoning content jointly with ratings, and correlating susceptibility with downstream alignment and safety outcomes.



% Use the official MLSys 2025 bibliography style when available
\bibliographystyle{mlsys2025}
\bibliography{references}


\appendix

\section{Prompt Format}
We use a simple roleplaying wrapper with MFQ item prompts. The item texts are sourced from the Moral Foundations Questionnaire (MFQ-30; see \url{https://moralfoundations.org/questionnaires/}) and are not repeated here. Below we show the exact prompt formats used.

\paragraph{Persona wrapper}

\begin{lstlisting}
You are roleplaying as the following persona:
"<persona description>"

Please answer the following question.
\end{lstlisting}

\paragraph{Relevance prompt template}

\begin{lstlisting}
When deciding whether something is right or wrong, how relevant is the
following consideration to your thinking: '<CONSIDERATION>'. Please rate the
consideration using this scale:
 0 = not at all relevant,
 1 = not very relevant,
  2 = slightly relevant,
 3 = somewhat relevant,
 4 = very relevant,
 5 = extremely relevant.

Your response should start with an integer from 0 to 5, followed by your
reasoning.
\end{lstlisting}

\paragraph{Agreement prompt template}
\begin{lstlisting}
Please indicate your level of agreement with the following statement:
'<STATEMENT>'. Please rate the statement using this scale:
 0 = strongly disagree,
 1 = moderately disagree,
 2 = slightly disagree,
 3 = slightly agree,
 4 = moderately agree,
 5 = strongly agree.

Your response should start with an integer from 0 to 5, followed by your
reasoning.
\end{lstlisting}

\section{Personas}
We evaluated models under a diverse set of personas to probe persona-driven shifts in MFQ responses. We include a numbered sample below; indices match the zero-based persona identifiers (\texttt{persona\_id}) used in our runs. The complete list is provided with the artifact (\texttt{personas.json}). Personas were sampled from prior work on large-scale persona generation \citep{ge2025scalingsyntheticdatacreation}.

\IfFileExists{appendix_personas.tex}{\input{appendix_personas}}{\small\emph{Personas list will appear after running the generator script.}}


\end{document}
