\documentclass{article}

% Allow builds from the repo root by searching both the current dir and articles/
\makeatletter
\def\input@path{{}{articles/}}
\makeatother

\usepackage{mlsys2025}
\usepackage{natbib}
\usepackage{url}
\usepackage{graphicx}
\graphicspath{{../results/}{results/}}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{placeins}
\usepackage{listings}
\usepackage{xurl} % better URL line-breaking
\usepackage[hidelinks]{hyperref} % load last

% Listings setup for prompt blocks (two-column friendly, no boxes)
\lstset{
  basicstyle=\ttfamily\scriptsize,
  breaklines=true,
  breakatwhitespace=false, % allow breaking long tokens
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  tabsize=2,
  frame=none,
  xleftmargin=0pt,
  xrightmargin=0pt,
  aboveskip=0.5em,
  belowskip=0.5em,
}


\title{Moral Robustness and Susceptibility\\ in Large Language Models}




% For double‑blind submission, leave author info empty under the MLSys style.
\author{Davi Bastos Costa, Felippe Alves \& Renato Vicente \\
TELUS Digital Research Hub\\ 
Center for Artificial Intelligence and Machine Learning\\
Institute of Mathematics, Statistics and Computer Science\\
University of São Paulo \\
\texttt{\{davi.costa,felippe.pereira,rvicente\}@usp.br} \\
}

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}
We study how persona conditioning influences the moral profile of large language models (LLMs). Using the Moral Foundations Questionnaire (MFQ), we elicit repeated ratings across diverse personas and models, and introduce a benchmark that quantifies two properties: (i) moral robustness (the stability of MFQ scores for personas under repeated sampling), and (ii) moral susceptibility (the sensitivity of MFQ scores under different personas). For moral robustness, model family explains most of the variance, and model size shows no systematic effect. In contrast, moral susceptibility exhibits a mild family effect but a clear within‑family size effect, with larger variants being more susceptible. We also qualitatively observe an inverse correlation between moral robustness and susceptibility, with more robust models tending to be less susceptible. Additionally, we display moral foundation profiles for models with no-persona conditioning and report moral foundation profiles for persona characterizations averaged across models, providing a complementary view of the moral effect of personas on model outputs.

\section{Introduction}
Reliable benchmarks for the social capabilities of large language models (LLMs) are crucial as models move into interactive, multi-agent settings where outcomes hinge on social intelligence. Recent evaluations probe theory-of-mind, negotiation under asymmetric information, cooperation, and deception through controlled role-play and game-theoretic tasks, e.g.: SOTOPIA for open-ended social interaction \cite{zhou2024sotopia}, MACHIAVELLI for reward–ethics trade-offs \cite{pan2023machiavelli}, NegotiationArena for bargaining \cite{bianchi2024negotiationarena}, ToMBench for structured ToM assessment \cite{chen-etal-2024-tombench}, and Mini-Mafia for emergent deception and detection \cite{costa2025deceivedetectdiscloselarge}. Complementary datasets benchmark social commonsense and moral judgment at scale \cite{sap-etal-2019-social,hendrycks2021ethics}. Motivated by this landscape, we focus on moral judgment as a core facet of social decision-making and alignment.

This paper introduces a benchmark based on the Moral Foundations Questionnaire \citep{moralfoundations2017questionnaires}, a widely used instrument in moral psychology that measures five moral foundations: Harm/Care, Fairness/Reciprocity, In-group/Loyalty, Authority/Respect, and Purity/Sanctity \citep{graham2009liberals,haidt2007when,moralfoundations2017questionnaires}. By eliciting LLMs to respond the MFQ questionary conditioned to different persona descriptions extracted from \citep{ge2025scalingsyntheticdatacreation}, we formalizes two complementary quantities: (i) moral robustness (the stability of MFQ scores for personas under repeated sampling) (ii) and moral susceptibility (the sensitivity of MFQ scores under different personas). These quantities are defined in Eq.~\eqref{eq:robustness} and Eq.~\eqref{eq:overall-susceptibility} respetively, both with foundation-level decompositions and uncertainty estimates.

Applying this framework across contemporary model families and sizes, we find that moral robustness variance is explained most by model family with no model size systematic effect. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible. In our experiments, Claude 4.5 Sonnet is the most and Grok 4 Fast the least robust. In contrast, Grok 4 Fast is the most and GPT-4o Mini the least susceptible. We qualitatively observe an inverse correlation between robustness and susceptibility.

Recent MFQ-based studies profile LLM value orientations and alignment. \citet{abdulhai-etal-2024-moral} adapt MFQ prompts to derive foundation scores. \citet{nunes2024hypocrites} combine MFQ with MFV to reveal inconsistencies between abstract and concrete judgments. \citet{aksoy2024whose} use MFQ-2 across eight languages to expose cultural/linguistic variability, and \citet{bajpai2024insights} compare MFQ-20 and moral competence between humans and chatbots. In parallel, MoralBench \citep{ji2025moralbenchmoralevaluationllms} offers a broad task suite; our MFQ persona framework complements it by isolating persona-driven shifts relative to a self baseline. For applied deployments, it remains useful to understand the baseline moral profile of the models being used; accordingly, we also report model-level MFQ profiles, complementing broad suites such as MoralBench and extending MFQ profiling to more advanced, state-of-the-art models. In addition, we provide MFQ profiles for different personas averaged across models to surface typical persona-driven shifts.

\section{Moral Robustness and Susceptibility Benchmark}

We define a benchmark to evaluate the moral robustness and moral susceptibility of LLMs. Moral robustness, is the stability of MFQ ratings across personas under repeated sampling, and moral susceptibility is the sensitivity of MFQ scores under different personas. These quantities are defined in Eq.~\eqref{eq:robustness} and Eq.~\eqref{eq:overall-susceptibility} respectively.

\subsection{Moral Foundation Questionnaire}

The Moral Foundation Questionnare \citep{moralfoundations2017questionnaires} comprises 30 questions split into two sections. The first includes 15 relevance judgments, which assess how relevant certain considerations are when deciding what is right or wrong, and the second includes 15 agreement statements, which measure the level of agreement with specific moral propositions \citep{graham2011mfq,moralfoundations2017questionnaires}. In both sections, respondents answer each item using an integer scale from 0 to 5, representing in the first section the perceived relevance of the consideration and in the second the degree of agreement with the statement (see Appendix~\ref{app:prompts} for a verbatim description including the interpretation of the scale). Questions map to five moral foundations: Harm/Care, Fairness/Reciprocity, In-group/Loyalty, Authority/Respect, Purity/Sanctity. The results are typically presented as foundation-level scores, obtained by averaging the ratings of the questions associated with each foundation.

Figure~\ref{fig:mfq-profiles} illustrates the resulting foundation-level MFQ scores across models using no-persona conditioning. Specifically, models were elicited to answer the 30 MFQ questions 10 times each, which we average by foundation and display with the corresponding standard error. Although not the focus of our work, understanding the moral profile of different frontier models is relevant, providing useful context for deployment and comparison.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../results/moral_foundations_relevance_profiles.png}
  \caption{Moral foundation profile across models with no-persona conditioning (self). Points show mean rating per foundation; error bars denote standard errors across questions within each foundation.}
  \label{fig:mfq-profiles}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../results/persona_moral_foundations_relevance_profiles.png}
  \caption{Moral foundation profiles for ten randomly selected personas, averaged across models. See the Appendix~\ref{app:personas} for the persona id--descriptions map.}
  \label{fig:persona-mfq-profiles}
\end{figure}

Figure~\ref{fig:persona-mfq-profiles} illustrates the resulting foundation-level MFQ scores average over all models for different personas. It gives an average characterization of the moral persona conditioning on models. The full per-persona, per-model and per-question MFQ ratings are available in our GitHub repository \cite{costa2025llmms}.

\subsection{Experimental Methodology}
For each model, we iterates through MFQ questions for a list of personas and repeat each question multiple times. Concretely we have:

\begin{itemize}
  \item \textbf{Personas:} We evaluate $|\mathcal{P}|=100$ persona descriptions drawn from prior work \citep{ge2025scalingsyntheticdatacreation}. Full persona descriptions and id--description map is provided in Appendix~\ref{app:personas}.
  \item \textbf{Prompting:} For each persona and question, the model receives a roleplaying instruction: ``You are roleplaying as the following persona:", followed by the persona description text and one of the $|\mathcal{Q}|=30$ MFQ questions.\footnote{We query one MFQ question at a time rather than the full questionnaire in a single prompt to avoid sequence- and order-dependent effects. Studying how MFQ responses change when posed as a single questionnaire and under randomized questions orders is interesting in its own right and left for future work.} We instruct the models to start their response with the rating (an integer from 0 to 5), followed by their reasoning. Exact prompt templates are provided in Appendix~\ref{app:prompts}.
  \item \textbf{Repetition:} Each persona--question pair is queried \(n=10\) times to estimate within-persona mean score and variance, which are then used to compute the moral robustness and susceptibility, defined in Eq.~\eqref{eq:robustness} and Eq.~\eqref{eq:overall-susceptibility}. See Section~\ref{sec:rating_estimation} for a discussion of the underlying problem and an outline of a more principled approach.
  \item \textbf{Decoding:} In the first run, we constrain outputs to begin with a single integer rating from 0 to 5, and parse this leading integer. Parsing failures are recorded and we repeat each attempt at most 4 times, allowing responses that do not begin with the rating (see Section~\ref{sec:failures} for more details). This approach minimizes costs and unexpectedly revealed that some personas more likely elicit models to not follow instruction (see Section~\ref{sec:unistructed_personas}).
  \item \textbf{Models:} We included: Claude Haiku 4.5, Claude Sonnet 4.5, Gemini 2.5 Flahs Lite, GPT-4.1, GPT-4.1 Mini, GPT-4.1 Nano, GPT-4o, GPT-4o Mini, Grok 4 and Grok 4 Fast.
  \item \textbf{Logging}: For each model we did a total of $|\mathcal{Q}|\times|\mathcal{P}|\times n =30\times 100\times 10=30.000$ requests. The resulting tables are available in our GitHub repository \cite{costa2025llmms}.
\end{itemize}


\subsection{Statistical Analysis}



This section formalizes the quantities we compute from the MFQ runs and how we summarize them into moral robustness and susceptibility metrics.

Let \(\mathcal{P}\) be the set of personas, \(\mathcal{Q}\) the set of 30 scored MFQ questions, and \(n\) the number of repeated queries per persona--question pair. For persona \(p\), question \(q\), and repetition \(i=1,\ldots,n\), let \(y_{pqi}\in\{0,\ldots,5\}\) be the parsed rating.

For each persona--question pair we compute the sample mean and the standard deviation across repetitions
\begin{align}
  \bar{y}_{pq} &= \frac{1}{n} \sum_{i=1}^{n} y_{pqi}, \label{eq:persona-question-mean}\\
  u_{pq} &= \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} \big(y_{pqi} - \bar{y}_{pq}\big)^2}, \label{eq:persona-question-se}
\end{align}

\paragraph{Moral robustness} We summarize within-pair variability by averaging the standard deviations in Eq.~\eqref{eq:persona-question-se} over personas and questions
\begin{equation}
  \bar{u} = \frac{1}{|\mathcal{P}|\,|\mathcal{Q}|} \sum_{p \in \mathcal{P}} \sum_{q \in \mathcal{Q}} u_{pq}.\label{eq:mean-uncertainty}
\end{equation}
Our robustness index is the reciprocal
\begin{equation}
  R = \frac{1}{\bar{u}}.\label{eq:robustness}
\end{equation}
Let the (sample) standard deviation of the \(u_{pq}\) values be
\begin{equation}
  s_u = \sqrt{\frac{1}{|\mathcal{P}|\,|\mathcal{Q}| - 1} \sum_{p \in \mathcal{P}} \sum_{q \in \mathcal{Q}} (u_{pq} - \bar{u})^2}.\label{eq:uncertainty-sd}
\end{equation}
Then the standard error of \(\bar{u}\) is \(\sigma_{\bar{u}} = s_u / \sqrt{|\mathcal{P}|\,|\mathcal{Q}|}\) which we propagate to get an estimate for the robustness standard error:
\begin{equation}
  \sigma_R = \frac{\sigma_{\bar{u}}}{\bar{u}^2}.
  \label{eq:robustness-se}
\end{equation}
Foundation-specific robustness reuse Eqs.~\eqref{eq:mean-uncertainty}--\eqref{eq:robustness-se} after restricting \(\mathcal{Q}\) to the question subset \(\mathcal{Q}_f\) for foundation \(f\).


\paragraph{Moral susceptibility} To stabilize estimates across many personas, we partition \(\mathcal{P}\) into \(G\) disjoint groups \(\mathcal{P}_1,\ldots,\mathcal{P}_G\) of equal size. For each question \(q\) and group \(g\), we compute the sample standard deviation of persona means
\begin{align}
  & s_{qg} = \sqrt{\frac{1}{|\mathcal{P}_g|-1} \sum_{p \in \mathcal{P}_g}(\bar{y}_{pq} - \bar{y}_{gq})^2},
  \label{eq:question-dispersion}
\end{align}
with $\bar{y}_{gq}$ the average over $\mathcal{P}_g$, i.e.: 
\begin{align}
  \bar{y}_{gq} = \frac{1}{|\mathcal{P}_g|} \sum_{p \in \mathcal{P}_g} \bar{y}_{pq}.
\end{align}
From $s_{qg}$ we obtain a group-level susceptibility sample
\begin{equation}
  S_g = \frac{1}{|\mathcal{Q}|} \sum_{q \in \mathcal{Q}} s_{qg}.\label{eq:group-susceptibility}
\end{equation}
The reported susceptibility is the mean over groups
\begin{equation}
  S = \frac{1}{G} \sum_{g=1}^{G} S_g,\label{eq:overall-susceptibility}
\end{equation}
with its standard error estimated from the between-group variability
\begin{equation}
  \sigma_S = \frac{1}{\sqrt{G}}\sqrt{\frac{1}{G-1} \sum_{g=1}^{G} (S_g - S)^2}.\label{eq:susceptibility-se}
\end{equation}
Foundation-specific susceptibilities reuse Eqs.~\eqref{eq:question-dispersion}--\eqref{eq:susceptibility-se} after restricting \(\mathcal{Q}\) to the question subset \(\mathcal{Q}_f\) for foundation \(f\).

\paragraph{Cross-model normalization} To facilitate comparison, we also present the $z$-scores that summarize relative performance across models. The $z$-score for moral metric $M\in \{S,R\}$ is
\begin{equation}
  z_{M} = \frac{M - \mu_M}{\sigma_M},
  \label{eq:zscore}
\end{equation}
where $M$ is the models's score, $\mu_M$ is the mean, and $\sigma_M$ is the standard deviation over different models. The uncertainty of $z_M$ is propagated from that of $M$, $\mu_M$ and $\sigma_M$.


\subsection{Average Score and Variance Estimation}
\label{sec:rating_estimation}

The first step to get the moral robustness and susceptibility is to compute the sample mean score and variance, Eq.~\eqref{eq:persona-question-mean} and Eq.~\eqref{eq:persona-question-se}. Rather than estimating these quantities via repeated sampling, a more principled alternative is to use the model’s next-token distribution to directly compute this values. Given the question prompt (that includes a the instruction that the response should begin with the rating from 0--5), let \(p_n = p(n\mid\text{prompt})\) denote the probability that the next token is the digit \(n\). Then, the average score and variance are given exactly by:
\begin{align}
  \mathbb{E}[n] = \sum_{n=0}^5 np_n, \quad \operatorname{Var}(n) = \sum_{n=0}^5 (n-\mathbb{E}[n])^2p_n
\end{align}
This is the average and variance that our 10-trial procedure approximates, while avoiding parsing failures. Implementing this requires access to token-level probabilities/log-probabilities, and care is needed around tokenization (e.g., space-prefixed digits or multiple token aliases).


\subsection{Failures to Respond}
\label{sec:failures}

In the first run, we constrain outputs to begin with a single integer rating from 0 to 5, and parse this leading integer. Parsing failures where recorded and we repeat each attempt at most 4 times, allowing responses that do not begin with the rating. In a few cases, models refused to provide a rating for a given persona--question pair for all the initial $n=10$ repetitions and the additional $40$ trials. Wheneve this happened we excluded these personas from our analysis, because we need a matrix with all valid entries to compute the susceptibility, Eq.~\eqref{eq:overall-susceptibility}, and its uncertainty, Eq.~\eqref{eq:susceptibility-se}.

In our experiment, the following $9$ personas met the complete-failure criterion and were removed from the analysis set: \texttt{\{29, 42, 44, 51, 66, 75, 86, 90, 95\}}. We then choosed the following grouping $|\mathcal{P}|-9=91= G\times |\mathcal{P}_G|=7 \times 13$ for estimating the moral susceptibility and its uncertainty.

Table~\ref{tab:failures_by_model} reports, for completeness, the total number of failed parsing rows and failed parsing attempts per model. The difference between the two columns gives a sense of the number of repetitions attempted. We list only models with non-zero totals. In the table, items with ``(self)" indicate the batch with no persona conditioning.

\begin{table}[t]
  \centering
  \caption{Total parsing failure counts per model.}
  \label{tab:failures_by_model}
  \begin{tabular}{lcc}
    \toprule
    Model & Failed rows & Total failures \\
    \midrule
    claude-sonnet-4-5 & 24 & 37  \\
    claude-sonnet-4-5 (self) & 213	& 213  \\
    gemini-2.5-flash-lite & 129 & 344 \\
    gemini-2.5-flash-lite (self) & 6 & 6 \\
    gpt-4.1 & 4 & 4   \\
    gpt-4.1 (self) & 13 & 51   \\
    gpt-4o & 24 & 37  \\
    gpt-4o (self) & 19 & 41  \\
    gpt-4o-mini & 71 & 202 \\
    gpt-4o-mini (self) & 18 & 38 \\
    grok-4 (self) & 5 & 5 \\
    \bottomrule
  \end{tabular}
\end{table}




\section{Results}

Our results for the moral robustness Eq.~\eqref{eq:robustness} and susceptibility Eq.~\eqref{eq:overall-susceptibility} by model, with $z$-score comparison Eq.~\eqref{eq:zscore}, is displayed in Table~\ref{tab:summary_by_model_with_z}. Qualitatively there appear to be an inverse correlation between moral robustness and susceptibility among families, with the Grok family the most susceptible and least robust, and the Claude family the most robust and one of the least susceptible.

% Merged summary + z-score table across models
\input{table_summary_by_model_with_z}

\subsection{Moral Robustness}

Our results for foundation-level moral robustness Eq.~\eqref{eq:robustness} is displayd in Figure~\ref{fig:robustness}. Moral robustness exhibits clear within-family structure across models. The Claude family is consistently the most robust, outperforming all other models by a sizeable margin across all foundations. In contrast, the Grok models are the least robust, underperforming all other models by a sizeable margin across all foundations. On the other hand, model size does not appear to have a systematic effect on moral robustness. These trends are visible in Figure~\ref{fig:robustness} and summarized in the $z$-score Table~\ref{tab:summary_by_model_with_z}.

% Robustness (overall + five foundations)
\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.3\linewidth]{../results/robustness_overall.png}\hfill
  \includegraphics[width=0.3\linewidth]{../results/robustness_harm_care.png}\hfill
  \includegraphics[width=0.3\linewidth]{../results/robustness_fairness_reciprocity.png}\\[0.75em]
  \includegraphics[width=0.3\linewidth]{../results/robustness_in_group_loyalty.png}\hfill
  \includegraphics[width=0.3\linewidth]{../results/robustness_authority_respect.png}\hfill
  \includegraphics[width=0.3\linewidth]{../results/robustness_purity_sanctity.png}
  \caption{Six-panel summary of robustness (inverse of average per-question standard deviation across repetitions). Top row: overall benchmark, Harm/Care, and Fairness/Reciprocity. Bottom row: In-group/Loyalty, Authority/Respect, and Purity/Sanctity. Error bars show propagated standard error via delta method; higher values indicate greater rating stability.}
  \label{fig:robustness}
\end{figure*}

\subsection{Moral Susceptibility}

Our results for foundation-level moral susceptibility Eq.~\ref{eq:overall-susceptibility} are displayed in Figure~\ref{fig:susceptibility}. Moral susceptibility exhibits a mild family effect as families tend to lie close together. However, there is a clear within-family size effect with larger variants having higher moral susceptibility. We refrain from fitting parametric trends versus model size because most model sizes are not publicly disclosed. These patterns are visible in Figure~\ref{fig:susceptibility} and summarized in the $z$-score Table~\ref{tab:summary_by_model_with_z}. The most susceptible model overall is Grok-4-fast and the least is GPT-4o Mini.

% Susceptibility (overall + five foundations)
\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.3\linewidth]{../results/susceptibility_overall.png}\hfill
  \includegraphics[width=0.3\linewidth]{../results/susceptibility_harm_care.png}\hfill
  \includegraphics[width=0.3\linewidth]{../results/susceptibility_fairness_reciprocity.png}\\[0.75em]
  \includegraphics[width=0.3\linewidth]{../results/susceptibility_in_group_loyalty.png}\hfill
  \includegraphics[width=0.3\linewidth]{../results/susceptibility_authority_respect.png}\hfill
  \includegraphics[width=0.3\linewidth]{../results/susceptibility_purity_sanctity.png}
  \caption{Six-panel summary of moral susceptibility (mean $\pm$ standard error across persona groups). Top row: overall benchmark, Harm/Care, and Fairness/Reciprocity. Bottom row: In-group/Loyalty, Authority/Respect, and Purity/Sanctity. Higher values indicate larger persona-driven shifts in MFQ subscale scores.}
  \label{fig:susceptibility}
\end{figure*}

\subsection{Uninstructed Personas}
\label{sec:unistructed_personas}

Some model's responses systematically ignore the leading integer prompt instruction (see Appendix~\ref{app:prompts} for prompt details). In most cases they open with text such as ``As a~\ldots'' before eventually providing a rating. Most cases were model--question specific. However, some personas appeared repeatedly accross models, and Table~\ref{tab:uninstructed-personas} highlights the two worst ``offenders" by aggregate parsing failures. This behavior was unexpected as their descriptions (see Appendix~\ref{app:personas}) do not obviously correlate with not following instructions, yet the pattern persists across architectures.

\begin{table}[t]
  \centering
  \caption{Personas with the highest counts parsing failures.}
  \label{tab:uninstructed-personas}
  \begin{tabular}{lrr}
    \toprule
    Persona id & 66 & 94 \\
    \midrule
    \texttt{gemini-2.5-flash-lite} & 30.0 & 58.0 \\
    \texttt{gpt-4o} & 6.0 & 4.0\\
    \texttt{gpt-4o-mini} & 60.0 & 30.0\\
    \midrule
    Total failures & 96.0 & 92.0\\
    \bottomrule
  \end{tabular}
\end{table}


\section{Conclusion}
We propose a principled benchmark for quantifying persona-driven shifts in LLM moral judgments using the MFQ. Our framework separates susceptibility (persona sensitivity) and robustness (rating stability), supports multiple model classes, and relies on transparent, easily repeatable procedures. Future work includes expanding persona taxonomies, stress-testing prompt formats, modeling reasoning content jointly with ratings, and correlating susceptibility with downstream alignment and safety outcomes.



% Use the official MLSys 2025 bibliography style when available
\bibliographystyle{mlsys2025}
\bibliography{references}


\appendix

\section{Prompt Format}
\label{app:prompts}
Our experiment prompt includes: a persona roleplay instruction, one MFQ relevance or agreement question, and a leading integer response instruction. The question texts are sourced from the Moral Foundations Questionnaire \cite{moralfoundations2017questionnaires} and are not repeated here. Below we show the exact prompt formats used.

\begin{itemize}
\item \textbf{Persona roleplay instruction}
\begin{lstlisting}
You are roleplaying as the following persona: 
<DESCRIPTION>

Please answer the following question.
\end{lstlisting}

\item \textbf{Relevance question template}

\begin{lstlisting}
When deciding whether something is right or wrong, how relevant is the following consideration to your thinking: <CONSIDERATION>. Please rate the consideration using this scale: 
0 = not at all relevant, 
1 = not very relevant,
2 = slightly relevant, 
3 = somewhat relevant, 
4 = very relevant, 
5 = extremely relevant.
\end{lstlisting}
\item \textbf{Agreement question template}
\begin{lstlisting}
Please indicate your level of agreement with the following statement: <STATEMENT>. Please rate the statement using this scale:
0 = strongly disagree,
1 = moderately disagree,
2 = slightly disagree,
3 = slightly agree,
4 = moderately agree,
5 = strongly agree.
\end{lstlisting}
\item \textbf{Leading integer response instruction}
\begin{lstlisting}
Your response should start with an integer from 0 to 5, followed by your reasoning.
\end{lstlisting}
\end{itemize}

The three bracketed words iterated respectively over: persona text descriptions (see Appendix~\ref{app:personas}); the 15 relevance MFQ questions, and the 15 agreemeent MFQ questions.



\section{Personas}
\label{app:personas}
We evaluated models under a diverse set of personas, $\mathcal{P}$, to probe persona-driven shifts in MFQ responses. We sampled $|\mathcal{P}|=100$ personas from prior work on large-scale persona generation \citep{ge2025scalingsyntheticdatacreation}. Below we enumerate each persona description, with the enumeration mapping description and persona id.

\IfFileExists{appendix_personas.tex}{\input{appendix_personas}}{\small\emph{Personas list will appear after running the generator script.}}

\end{document}
