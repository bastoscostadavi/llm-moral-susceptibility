\documentclass{article}

\PassOptionsToPackage{numbers,compress}{natbib}

% Allow builds from the repo root by searching both the current dir and articles/
\makeatletter
\def\input@path{{}{articles/}}
\makeatother

\usepackage[preprint]{neurips_2025}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{placeins}
\usepackage{listings}
\usepackage{xurl} % better URL line-breaking

\usepackage{fontawesome5}

\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,fit,calc,backgrounds}




\graphicspath{{../results/}{results/}}

% Listings setup for prompt blocks (two-column friendly, no boxes)
\lstset{
  basicstyle=\\ttfamily\\scriptsize,
  breaklines=true,
  breakatwhitespace=false, % allow breaking long tokens
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  tabsize=2,
  frame=none,
  xleftmargin=0pt,
  xrightmargin=0pt,
  aboveskip=0.5em,
  belowskip=0.5em,
}

\title{Moral Susceptibility and Robustness under Persona~Role-Play in Large Language Models}




% For double-blind submission, replace the author block below with anonymized information.
\author{Davi Bastos Costa, Felippe Alves \& Renato Vicente \\
TELUS Digital Research Hub\\ 
Center for Artificial Intelligence and Machine Learning\\
Institute of Mathematics, Statistics and Computer Science\\
University of São Paulo \\
\texttt{\{davi.costa,felippe.pereira,rvicente\}@usp.br} \\
}

\begin{document}

\maketitle

\begin{abstract}
  Large language models (LLMs) increasingly operate in social contexts, motivating analysis of how they express and shift moral judgments. In this work, we investigate the moral response of LLMs to persona role-play, prompting a LLM to assume a specific character. Using the Moral Foundations Questionnaire (MFQ), we introduce a benchmark that quantifies two properties: moral susceptibility and moral robustness, defined from the variability of MFQ scores across and within personas, respectively. We find that, for moral robustness, model family accounts for most of the variance, while model size shows no systematic effect. The Claude family is, by a significant margin, the most robust, followed by Gemini and GPT-4 models, with other families exhibiting lower robustness. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible. Moreover, robustness and susceptibility are positively correlated, an association that is more pronounced at the family level. Additionally, we present moral foundation profiles for models without persona role-play and for personas averaged across models. Together, these analyses provide a systematic view of how persona conditioning shapes moral behavior in large language models.
\end{abstract}


\section{Introduction}
As large language models (LLMs) move into interactive, multi-agent settings, reliable benchmarks for their social reasoning are essential. Recent evaluations probe theory-of-mind, multi-agent interactions under asymmetric information, cooperation, and deception through controlled role-play and game-theoretic tasks \cite{zhou2024sotopia,pan2023machiavelli,bianchi2024negotiationarena,chen-etal-2024-tombench,costa2025deceivedetectdiscloselarge}. Complementary datasets benchmark social commonsense, moral judgment, and self-recognition capabilities \cite{sap-etal-2019-social,hendrycks2021ethics,bai2025knowthyselfincapabilityimplications}. Motivated by this landscape, we focus on moral judgment as a core facet of social decision-making and alignment.

This paper introduces a benchmark that combines persona role-play---prompting a LLM to assume a specific character---with the Moral Foundations Questionnaire \citep{moralfoundations2017questionnaires}, a widely used instrument in moral psychology that measures five moral foundations: Harm/Care, Fairness/Reciprocity, In-group/Loyalty, Authority/Respect, and Purity/Sanctity \citep{graham2009liberals,haidt2007when,moralfoundations2017questionnaires}. We elicit LLMs to respond to the MFQ while role-playing personas drawn from \citet{ge2025scalingsyntheticdatacreation}. From these responses, we define two complementary quantities: moral robustness, the stability of MFQ scores over personas under repeated sampling, and moral susceptibility, the sensitivity of MFQ scores to persona variation. See Fig.~\ref{fig:mfq-susceptibility-robustness} for a conceptual overview diagram. These metrics are defined in Eq.~\eqref{eq:robustness} and Eq.~\eqref{eq:overall-susceptibility}, each with foundation-level decompositions and uncertainty estimates. 

\begin{figure}[t]
  \centering
  \definecolor{suscolor}{HTML}{4A90E2} % Gemini palette
  \definecolor{robcolor}{HTML}{E67E22} % Claude-Sonnet palette
  \definecolor{promptcolor}{HTML}{4A90E2}
  \definecolor{modelcolor}{HTML}{E67E22}
  \definecolor{ratingcolor}{HTML}{52B788}
  \definecolor{datacolor}{HTML}{F9E79F}
  \definecolor{susbgcolor}{HTML}{BDA0E3}
  \begin{tikzpicture}[
    font=\small,
    node distance=12mm and 18mm,
    % styles
    box/.style = {draw, rounded corners, inner sep=6pt, align=left},
    persona/.style = {draw=modelcolor!50!black, rounded corners=3pt, inner xsep=6pt, inner ysep=3pt, align=left, text=black, fill=modelcolor!25},
    title/.style = {font=\bfseries},
    arrow/.style = {line width=1pt, -{Latex[length=2.4mm,width=1.4mm]}},
  ]
  
  % Persona stack (center)
  \matrix (pers) [column sep=0mm, row sep=2mm] {
    \node[persona] (p0) {\faUser\ \ Persona $0$}; \\
    \node[persona] (p1) {\faUser\ \ Persona $1$}; \\
    \node[inner ysep=3pt, inner xsep=2pt] (dots) {$\vdots$}; \\
    \node[persona] (plast) {\faUser\ \ Persona $|\mathcal{P}|$}; \\
  };

  % Susceptibility box around personas (background layer so text stays visible)
  \begin{pgfonlayer}{background}
    \node[draw=susbgcolor!70!black, rounded corners, fill=susbgcolor!40,
          fit=(p0) (p1) (dots) (plast), inner xsep=12pt, inner ysep=10pt,
          label={[title]above:Moral Susceptibility},
          label={[text=black]below:Across-persona variability}] (susbox) {};
  \end{pgfonlayer}

  % Data block (vertical)
  \node[draw=black, rounded corners, inner xsep=6pt, inner ysep=6pt, align=center, fill=datacolor!70, label={[title]above:Model}] (modelbox) [left=32mm of susbox] {
    \begin{tikzpicture}[scale=0.42, line width=0.45pt]
      \foreach \y [count=\i] in {0.75,-0.35,-1.45} {
        \node[draw,circle,inner sep=1.2pt] (in\i) at (0,\y) {};
      }
      \foreach \y [count=\i] in {0.75,-0.35,-1.45} {
        \node[draw,circle,inner sep=1.2pt] (hid\i) at (1,\y) {};
      }
      \foreach \y [count=\i] in {0.2,-0.9} {
        \node[draw,circle,inner sep=1.2pt] (out\i) at (2,\y) {};
      }
      \foreach \i in {1,2,3} {
        \foreach \j in {1,2,3} {
          \draw (in\i) -- (hid\j);
        }
      }
      \foreach \i in {1,2,3} {
        \foreach \j in {1,2} {
          \draw (hid\i) -- (out\j);
        }
      }
    \end{tikzpicture}
  };

  \node[draw=black, rounded corners, inner xsep=6pt, inner ysep=6pt, align=left, fill=promptcolor!18, label={[title]above:Prompt}] (promptbox) [above=10mm of modelbox] {
    \begin{tabular}{@{}l@{}}
      \strut \faUser\ Persona\\[4pt]
      \strut \faFile\ MFQ
    \end{tabular}
  };

  \node[draw=black, rounded corners, inner xsep=6pt, inner ysep=6pt, align=center, fill=ratingcolor!22, label={[title]above:MFQ Scores}] (scorebox) [below=14mm of modelbox] {
    \strut $0,1,2,3,4,5$
  };

  \coordinate (promptarrowstart) at ($(promptbox.south)+(0,-8pt)$);
  \coordinate (modelarrowend) at ($(modelbox.north)+(0,8pt)$);
  \coordinate (modelarrowstart) at ($(modelbox.south)+(0,-13pt)$);
  \coordinate (scorearrowend) at ($(scorebox.north)+(0,13pt)$);
  \draw[arrow] (promptarrowstart) -- (modelarrowend);
  \draw[arrow] (modelarrowstart) -- (scorearrowend);

  % Runs box (right)
  \node[box, draw=robcolor, text=black, fill=modelcolor!14, right=28mm of susbox, anchor=center, inner xsep=10pt, align=center] (runs) {%
    \strut \faRedo \ \ Run $1$\\[-1pt]
    \strut \faRedo \ \ Run $2$\\[-1pt]
    \strut $\vdots$\\[-1pt]
    \strut \faRedo \ \ Run $n$
  };

  % Robustness title above runs
  \node[title, text=black, above=2mm of runs] {Moral Robustness};
  \node[text=black, below=2mm of runs] {Within-persona variability};
  
  % Merge point for persona arrows before entering robustness box
  \coordinate (merge) at ($(runs.center)+(-6mm,0)$);
  \coordinate (p0merge) at ($(p0.east)!0.88!(merge)$);
  \coordinate (p1merge) at ($(p1.east)!0.88!(merge)$);
  \coordinate (plastmerge) at ($(plast.east)!0.88!(merge)$);
  \coordinate (runsentry) at ($(runs.center)+(-1.5mm,0)$);

  % Persona arrows converge at merge point, leaving a small gap before merging
  \draw[arrow] (p0.east) -- (p0merge);
  \draw[arrow] (p1.east) -- (p1merge);
  \draw[arrow] (plast.east) -- (plastmerge);

  % Overall analysis grouping
  \coordinate (datatop) at ($(promptbox.north)+(0,6mm)$);
  \coordinate (databottom) at ($(scorebox.south)+(0,-6mm)$);

  \begin{pgfonlayer}{background}
    \node[draw=black, rounded corners, fit=(promptbox) (modelbox) (scorebox) (datatop) (databottom), inner xsep=8pt, inner ysep=0pt, label={[title]below:Data}] (databox) {};
    \node[draw=black, rounded corners, line width=0.9pt, fit=(susbox) (runs), inner xsep=28pt, inner ysep=26pt, label={[title]below:Benchmark}] (analysisbox) {};
  \end{pgfonlayer}

  
  \end{tikzpicture}
  \caption{Left: summary of our data collection pipeline: we elicit models to respond to the MFQ conditioned to a persona. Right: summary of our benchmark pipeline: robustness,  Eq.~\ref{eq:robustness}, and susceptibility, Eq.~\ref{eq:overall-susceptibility}, are computed from across and within persona variability in MFQ scores.}
  \label{fig:mfq-susceptibility-robustness}
  \end{figure}
  

Applying this framework across contemporary model families and sizes, we find that model family accounts for most of the variance in moral robustness, with no systematic effect of model size. In contrast, moral susceptibility shows a mild family effect but a clear within-family size trend, with larger variants being more susceptible. Among individual models, Claude 4.5 Sonnet is the most robust and Grok 4 Fast the least. Conversely, Gemini 2.5 Flash is the most susceptible, while GPT-5 Nano is the least. Overall, we observe a a positive correlation between robustness and susceptibility, which is more pronounced at the family level, as seen in Section~\ref{sec:robustness-susceptibility-correlation}.

Recent research has examined the moral and social behavior of LLMs through the lens of the MFQ, exploring their value orientations, cultural variability, and alignment with human moral judgments \citep{abdulhai-etal-2024-moral,nunes2024hypocrites,aksoy2024whose,bajpai2024insights,ji2025moralbenchmoralevaluationllms}. Parallel efforts study persona role-playing as a mechanism for conditioning model behavior, including benchmarks, interactive environments, and diagnostic analyses \citep{tseng2024two,wang2023rolellm,samuel2025personagym,yu2025rpgbench,xu2024character,elboudouri2025rpeval,bai2025concept}. Our MFQ persona framework bridges these directions by systematically quantifying how persona conditioning alters moral judgments, separating the effects of repeated sampling (moral robustness) from those of persona variation (moral susceptibility). In addition, we report MFQ profiles for both unconditioned and persona-conditioned settings, providing a comparative view of baseline moral tendencies and persona-driven moral shifts across models.

\section{Moral Robustness and Susceptibility Benchmark}

We define a benchmark to evaluate the moral robustness and moral susceptibility of LLMs. Moral robustness is the stability of MFQ ratings across personas under repeated sampling, and moral susceptibility is the sensitivity of MFQ scores under different personas. These quantities are defined in Eq.~\eqref{eq:robustness} and Eq.~\eqref{eq:overall-susceptibility} respectively.

\subsection{Moral Foundation Questionnaire}

The Moral Foundations Questionnaire \citep{moralfoundations2017questionnaires} is a widely used instrument in moral psychology \citep{graham2009liberals,haidt2007when,moralfoundations2017questionnaires} and comprises 30 questions split into two sections. The first includes 15 relevance judgments, which assess how relevant certain considerations are when deciding what is right or wrong, and the second includes 15 agreement statements, which measure the level of agreement with specific moral propositions \citep{graham2011mfq,moralfoundations2017questionnaires}. In both sections, respondents answer each item using an integer scale from 0 to 5, representing in the first section the perceived relevance of the consideration and in the second the degree of agreement with the statement (see Appendix~\ref{app:prompts} for a verbatim description including the interpretation of the scale). Questions map to five moral foundations: Harm/Care, Fairness/Reciprocity, In-group/Loyalty, Authority/Respect, Purity/Sanctity. The results are typically presented as foundation-level scores, obtained by averaging the ratings of the questions associated with each foundation.

Figure~\ref{fig:mfq-profiles} illustrates the resulting foundation-level MFQ scores across models using no-persona role-play. Models were elicited to answer the 30 MFQ questions 10 times each, which we average by foundation and display with the corresponding standard error. Although not the focus of our work, understanding the moral profile of different frontier models is relevant, providing useful context for deployment and comparison.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../results/moral_foundations_relevance_profiles.pdf}
  \caption{Moral foundation profile across models with no-persona role-play (self) together with the average over all models. Points show mean rating and error bars denote standard errors across questions within each foundation. See Table~\ref{tab:moral_foundations_profiles} for exact values.}
  \label{fig:mfq-profiles}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{../results/persona_moral_foundations_relevance_profiles.pdf}
  \caption{Moral foundation profiles for fourteen randomly selected personas  averaged across models together with the average over all personas and models. Points show mean rating and error bars denote standard errors across questions within each foundation. See Table~\ref{tab:persona_moral_foundations_profiles} for exact values.}
  \label{fig:persona-mfq-profiles}
\end{figure}

Figure~\ref{fig:persona-mfq-profiles} reports foundation-level MFQ scores averaged over all models for different personas, and for the average persona. It gives an average characterization of the moral profile of models elicited by a given persona (see Appendix~\ref{app:personas} for the persona descriptions). It is intereting to compare the average score across models with no persona-conditioning with the average score across models over all personas, this gives a sense of the average moral response of models to persona role-play (see Appendix~\ref{app:moral-foundations-tables} for details). Complementary, in Table~\ref{tab:persona_foundation_maxima}, we present the persona ID that attained the highest MFQ score averaged across models for each foundation. The full per-persona, per-model, and per-question MFQ ratings are available in our GitHub repository \cite{costa2025llmms}.

\begin{table}[t]
  \centering
  \caption{Persona that attains the highest MFQ score averaged across models for each foundation.}
  \label{tab:persona_foundation_maxima}
  \resizebox{\linewidth}{!}{\begin{tabular}{lccccc}
    \toprule
    & Harm/Care & Fairness/Reciprocity & In-group/Loyalty & Authority/Respect & Purity/Sanctity \\
    \midrule
    Persona ID & 12 & 27 & 75 & 25 & 76 \\
    Mean Score & 4.79 & 4.84 & 4.72 & 4.54 & 4.42 \\
    \bottomrule
  \end{tabular}}
\end{table}

\subsection{Experimental Methodology}
\label{sec:experimental-methodology}
For each model, we iterate through all MFQ questions for every persona, repeating each question multiple times. Concretely we have:

\begin{itemize}
  \item \textbf{Personas:} We evaluate $|\mathcal{P}|=100$ persona descriptions drawn from prior work \citep{ge2025scalingsyntheticdatacreation}. Full persona descriptions and the corresponding ID–description mappings are provided in Appendix~\ref{app:personas}.
  \item \textbf{Prompting:} For each persona and question, the model receives a role-playing instruction: ``You are roleplaying as the following persona:", followed by the persona description text and one of the $|\mathcal{Q}|=30$ MFQ questions.\footnote{We query one MFQ question at a time rather than the full questionnaire in a single prompt to avoid sequence- and order-dependent effects.} We instruct the models to start their response with the rating (an integer from 0 to 5), followed by their reasoning. Exact prompt templates are provided in Appendix~\ref{app:prompts}.
  \item \textbf{Repetition:} Each persona--question pair is queried \(n=10\) times to estimate within-persona mean score and variance, which are then used to compute the moral robustness and susceptibility, defined in Eq.~\eqref{eq:robustness} and Eq.~\eqref{eq:overall-susceptibility}. See Section~\ref{sec:rating_estimation} for a discussion of the underlying problem and an outline of a more principled approach.
  \item \textbf{Decoding:} In the first run, we constrain outputs to begin with a single integer rating from 0 to 5, and parse this leading integer. Parsing failures are recorded and we repeat each attempt at most 4 times, allowing responses that do not begin with the rating (see Section~\ref{sec:failures} for more details). This approach minimizes costs and unexpectedly revealed that some personas more likely elicit models to not follow instructions (see Section~\ref{sec:failures}).
  \item \textbf{Models:} We included: Claude Haiku 4.5, Claude Sonnet 4.5, DeepSeek V3,DeepSeek V3.1, Gemini 2.5 Flash Lite, Gemini 2.5 Flash, GPT-4.1, GPT-4.1 Mini, GPT-4.1 Nano, GPT-4o, GPT-4o Mini, GPT-5, GPT-5 Mini, GPT-5 Nano, Grok 4 and Grok 4 Fast. \item \textbf{Families:} We group the above models in the following families: Claude, DeepSeek, Gemini, GPT-4, GPT-5 and Grok.
  \item \textbf{Logging}: For each model we did a total of $|\mathcal{Q}|\times|\mathcal{P}|\times n =30\times 100\times 10=30,000$ requests. The resulting tables are available in our GitHub repository \cite{costa2025llmms}.
\end{itemize}
We next formalize how these repeated ratings are aggregated into moral robustness and susceptibility scores.

\subsection{Statistical Analysis}

This section formalizes the quantities we compute from the MFQ runs and how we summarize them into moral robustness and susceptibility metrics.

Let \(\mathcal{P}\) be the set of personas, \(\mathcal{Q}\) the set of 30 scored MFQ questions, and \(n\) the number of repeated queries per persona--question pair. For persona \(p\), question \(q\), and repetition \(i=1,\ldots,n\), let \(y_{pqi}\in\{0,\ldots,5\}\) be the parsed rating.

For each persona--question pair we compute the sample mean and the standard deviation across repetitions
\begin{align}
  \bar{y}_{pq} = \frac{1}{n} \sum_{i=1}^{n} y_{pqi},\qquad u_{pq}^2 = \frac{1}{n-1} \sum_{i=1}^{n} \big(y_{pqi} - \bar{y}_{pq}\big)^2.
  \label{eq:persona-question-mean}
\end{align}

\paragraph{Moral robustness} We summarize within-persona variability by averaging the standard deviations in Eq.~\eqref{eq:persona-question-mean} over personas and questions and we estimate its uncertainty via the sample standard error: 
\begin{equation}
  \bar{u} = \frac{1}{|\mathcal{P}|\,|\mathcal{Q}|} \sum_{p \in \mathcal{P}} \sum_{q \in \mathcal{Q}} u_{pq},\qquad \sigma_{\overline{u}}^2 = \frac{1}{|\mathcal{P}||\mathcal{Q}|(|\mathcal{P}|\,|\mathcal{Q}|-1)} \sum_{p \in \mathcal{P}} \sum_{q \in \mathcal{Q}} (u_{pq} - \bar{u})^2.\label{eq:mean-uncertainty}
\end{equation}
We define the unbounded robustness index as the inverse:
\begin{equation}
  \widetilde{R} = \frac{1}{\bar{u}},\qquad \sigma_{\widetilde{R}} = \frac{\sigma_{\bar{u}}}{\bar{u}^2},
  \label{eq:unbounded-robustness}
\end{equation}
with uncertainty approximated by propagating $\sigma_{\bar{u}}$ from $\bar{u}$.
Moral robustness is then defined as
\begin{equation}
  R = \frac{\widetilde{R}}{\widetilde{R} + \mathbb{E}[\widetilde{R}]},\qquad
  \sigma_R = \frac{\mathbb{E}[\widetilde{R}]}{(\widetilde{R}+\mathbb{E}[\widetilde{R}])^2} \, \sigma_{\widetilde{R}},
  \label{eq:robustness}
\end{equation}
where the average $\mathbb{E}[\widetilde{R}]$ is computed across models. As defined, $R$ is dimensionless, $R\in [0, 1]$ and $R = 1/2$ sets the threshold for being more robust (smaller within-persona variability) than the overall average, mirroring the susceptibility interpretation below.


\paragraph{Moral susceptibility} For our across-perona variability index we partition \(\mathcal{P}\) into \(G\) disjoint groups \(\mathcal{P}_1,\ldots,\mathcal{P}_G\) of equal size. For each question \(q\) and group \(g\), we compute the sample standard deviation of persona means
\begin{align}
  & s_{qg}^2 = \frac{1}{|\mathcal{P}_g|-1} \sum_{p \in \mathcal{P}_g}(\bar{y}_{pq} - \bar{y}_{gq})^2,\qquad \bar{y}_{gq} = \frac{1}{|\mathcal{P}_g|} \sum_{p \in \mathcal{P}_g} \bar{y}_{pq}.
  \label{eq:question-dispersion}
\end{align}
From $s_{qg}$ we obtain the unbounded susceptibility as the average over groups of group-level susceptibility samples:
\begin{equation}
  \widetilde{S} = \frac{1}{G} \sum_{g=1}^{G} \widetilde{S}_g,\qquad\widetilde{S}_g = \frac{1}{|\mathcal{Q}|} \sum_{q \in \mathcal{Q}} s_{qg},\qquad \sigma_{\widetilde{S}} = \sqrt{\frac{1}{G(G-1)} \sum_{g=1}^{G} (\widetilde{S}_g - \widetilde{S})^2},
  \label{eq:unbounded-susceptibility}
\end{equation}
with its standard error estimated from the between-group variability. Analogously to robustness we define the bounded susceptibility as
\begin{equation}
  S = \frac{\widetilde{S}}{\widetilde{S}+ \mathbb{E}[\widetilde{S}]},\qquad
  \sigma_S = \frac{\mathbb{E}[\widetilde{S}]}{(\widetilde{S}+ \mathbb{E}[\widetilde{S}])^2} \, \sigma_{\widetilde{S}},
  \label{eq:overall-susceptibility}
\end{equation}
where the average $\mathbb{E}[\widetilde{S}]$ is computed across models. In analogy with the robustness index,  $S$ is dimensionless, \(S\in [0,1]\) and \(S=1/2\) marks the benchmark mean.

To propagate uncertainty in both robustness and susceptibility we adopt a first-order approximation: we treat the cross-model averages $\mathbb{E}[\bar{u}]$ and $\mathbb{E}[\widetilde{S}]$ as fixed constants and only propagate the uncertainty from $\widetilde{R}$ and $\widetilde{S}$. Linearizing Eqs.~\eqref{eq:robustness} and \eqref{eq:overall-susceptibility} around these values yields the closed-form standard errors $\sigma_R$ and $\sigma_S$ reported alongside each index. This analytical approximation was compared with a bootstrap approach, and both gave similar values.

Foundation-specific robustness and susceptibilities reuse Eqs.~\eqref{eq:mean-uncertainty}--\eqref{eq:overall-susceptibility} after restricting \(\mathcal{Q}\) to the question subset \(\mathcal{Q}_f\) for foundation \(f\).

\subsection{Average Score and Variance Estimation}
\label{sec:rating_estimation}

The first step to get the moral robustness and susceptibility is to compute the sample mean score and variance, Eq.~\eqref{eq:persona-question-mean}. Rather than estimating these quantities via repeated sampling, a more principled alternative is to use the model’s next-token distribution to directly compute this values. Given the question prompt (that includes a the instruction that the response should begin with the rating from 0--5), let \(p_n = p(n\mid\text{prompt})\) denote the probability that the next token is the digit \(n\). Then, the average score and variance are given exactly by:
\begin{align}
  \mathbb{E}[n] = \sum_{n=0}^5 np_n, \quad \operatorname{Var}(n) = \sum_{n=0}^5 (n-\mathbb{E}[n])^2p_n
\end{align}
This is the average and variance that our 10-trial procedure approximates, while avoiding parsing failures. Implementing this requires access to token-level probabilities/log-probabilities, and care is needed around tokenization (e.g., space-prefixed digits or multiple token aliases).




\section{Results}


\begin{table}[t]
  \centering
  \caption{Cross-model means of the unbounded robustness and susceptibility per foundation.}
  \label{tab:unbounded_means}
  \begin{tabular}{lcc}
    \toprule
    Foundation & $\mathbb{E}[\widetilde{R}]$ & $\mathbb{E}[\widetilde{S}]$ \\
    \midrule
    All foundations & $18 \pm 7$ & $0.62 \pm 0.02$ \\
    Authority/Respect & $16 \pm 6$ & $0.70 \pm 0.03$ \\
    Fairness/Reciprocity & $29 \pm 14$ & $0.42 \pm 0.02$ \\
    Harm/Care & $27 \pm 12$ & $0.48 \pm 0.03$ \\
    In-group/Loyalty & $18 \pm 7$ & $0.75 \pm 0.03$ \\
    Purity/Sanctity & $17 \pm 7$ & $0.76 \pm 0.03$ \\
    \bottomrule
  \end{tabular}
\end{table}


Our results for the overall moral robustness, Eq.~\eqref{eq:robustness}, and moral susceptibility, Eq.~\eqref{eq:overall-susceptibility}, by model are displayed in Figure~\ref{fig:robustness-susceptibility}. For robustness, we see that model family explains most of the variance, with model size having no systematic effect. The Claude family is by a significant margin the most robust, while Grok are the least. At the model level Claude Sonnet 4.5 stand out as the most robust and GPT-5 Nano as the least. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger vairants being more susceptible. At the model level, Gemini 2.5 Flash is the most suscepbtile and GPT-5 Nano the least. Overall, the Grok family sits as the primary outlier, pairing comparatively low robustness with high susceptibility.

% Overall Susceptibility and Robustness
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=\linewidth]{../results/moral_metrics_overall_bars.pdf}
  \caption{Left: moral robustness, Eq.~\eqref{eq:robustness}: higher values indicate greater MFQ rating stability. Right: moral susceptibility, Eq.~\eqref{eq:overall-susceptibility}: higher values indicate larger persona-driven shifts in MFQ scores.}
  \label{fig:robustness-susceptibility}
\end{figure}

In addition to moral robustness and susceptibility, we also display in Table~\ref{tab:unbounded_means} the average value across models of the unbounded robustness Eq.~\eqref{eq:unbounded-robustness} and susceptibility Eq.~\eqref{eq:unbounded-susceptibility}. As one can see, Fairness/Reciprocity and Harm/Care yield the highest average unbounded robustness and the lowest unbounded susceptibility, compared with other foudnations.

\subsection{Moral Robustness}

Our results for foundation-level moral robustness Eq.~\eqref{eq:robustness} are displayed in Figure~\ref{fig:robustness-foundations}. One can see that models have different moral profiles as measured by robustness, with the index taking different values per foundation relative to one another. For most families, there is a resemblance on the moral robustness profile. This is not the case for Claude, and the resemblance desapears as one goes to the nano version.



\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{../results/robustness_bars.pdf}
  \caption{Moral robustness foundation profile across models, Eq.~\eqref{eq:robustness}: higher values indicate greater MFQ rating stability.}
  \label{fig:robustness-foundations}
\end{figure}

\subsection{Moral Susceptibility}

Our results for foundation-level moral susceptibility Eq.~\ref{eq:overall-susceptibility} are displayed in Figure~\ref{fig:susceptibility-foundations}. One can see that
models have a more balanced susceptibility moral profile if compared with robustness, with no  model scoring significantly higher across foundations. Interestingly, DeepSeek V3.1 and the Llama models have a more similar susceptibility profile, with low Harm/Care and Fairness/Reciprocity, in comparison with the other foundations. In contrast, Gemini-2.5 Flahs Lite, GPT-4.1 Nano, GPT-5 Nano, have a high Harm/Care and Fairness/Reciprocity.


\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{../results/susceptibility_bars.pdf}
  \caption{Moral susceptibility foundation profile across models, Eq.~\eqref{eq:overall-susceptibility}: higher values indicate larger persona-driven shifts in MFQ scores.}
  \label{fig:susceptibility-foundations}
\end{figure}


\subsection{Correlation Between Robustness and Susceptibility}
\label{sec:robustness-susceptibility-correlation}

Table~\ref{tab:correlation_foundations} summarises the Pearson correlations from Eq.~\eqref{eq:pearson} at the model and family levels. With all models included we see a positive association between robustness and susceptibility ($+0.27 \pm 0.07$ across models, $+0.29 \pm 0.09$ by family), with Fairness/Reciprocity and Harm/Care showing the strongest dependencies while Purity/Sanctity remains negative. The Grok family suppresses these values because of its low robustness and high susceptibility, particularly within Purity/Sanctity. Excluding Grok lifts every foundation, reinforcing the positive trends without altering the qualitative ordering across foundations.

\begin{table}[t]
  \centering
  \caption{Pearson correlation between robustness and susceptibility overall and by foundation. Columns on the right report the same metrics after excluding the Grok family.}
  \label{tab:correlation_foundations}
  \begin{tabular}{lcccc}
    \toprule
    & \multicolumn{2}{c}{All models} & \multicolumn{2}{c}{Excluding Grok} \\
    \cmidrule(lr){2-3} \cmidrule(l){4-5}
    Foundation & Model $r_{RS}$ & Family $\overline{r}_{RS}$ & Model $r_{RS}$ & Family $\overline{r}_{RS}$ \\
    \midrule
    All foundations & $+0.27 \pm 0.07$ & $+0.29 \pm 0.09$ & $+0.41 \pm 0.07$ & $+0.48 \pm 0.09$ \\
    Authority/Respect & $+0.07 \pm 0.09$ & $+0.12 \pm 0.14$ & $+0.20 \pm 0.10$ & $+0.33 \pm 0.16$ \\
    Fairness/Reciprocity & $+0.36 \pm 0.08$ & $+0.46 \pm 0.10$ & $+0.45 \pm 0.08$ & $+0.62 \pm 0.09$ \\
    Harm/Care & $+0.26 \pm 0.06$ & $+0.39 \pm 0.07$ & $+0.38 \pm 0.06$ & $+0.59 \pm 0.07$ \\
    In-group/Loyalty & $+0.25 \pm 0.07$ & $+0.41 \pm 0.10$ & $+0.31 \pm 0.07$ & $+0.50 \pm 0.10$ \\
    Purity/Sanctity & $-0.18 \pm 0.09$ & $-0.24 \pm 0.11$ & $-0.10 \pm 0.09$ & $-0.15 \pm 0.12$ \\
    \bottomrule
  \end{tabular}
\end{table}


\section{Conclusion}
We present a benchmark for evaluating large language models's moral response to persona role-play using the Moral Foundations Questionnaire. By distinguishing moral robustness (within-persona variability) from moral susceptibility (across-persona variability), our results reveal consistent family-level patterns for robutness and a size-dependent susceptibility trends. Together, these results offer a systematic framework for comparing moral profiles across model families and sizes, providing a quantitative basis for future studies of moral behavior in language models.


\section*{Acknowledgments}

We gratefully acknowledge the financial support of the TELUS Digital Research Hub.


\bibliographystyle{plainnat}
\bibliography{references.bib}


\appendix

\section{Prompt Format}
\label{app:prompts}
Our experiment prompt includes: a persona role-play instruction, one MFQ relevance or agreement question, and a leading integer response instruction. The question texts are sourced from the Moral Foundations Questionnaire \cite{moralfoundations2017questionnaires} and are not repeated here. Below we show the exact prompt formats used.

\textbf{Persona role-play instruction.}
\begin{quote}
\ttfamily\scriptsize
You are roleplaying as the following persona: 
<DESCRIPTION>

Please answer the following question.
\end{quote}

\textbf{Relevance question template.}
\begin{quote}
\ttfamily\scriptsize
When deciding whether something is right or wrong, how relevant is the following consideration to your thinking: <CONSIDERATION>. Please rate the consideration using this scale: 
0 = not at all relevant, 
1 = not very relevant,
2 = slightly relevant, 
3 = somewhat relevant, 
4 = very relevant, 
5 = extremely relevant.
\end{quote}

\textbf{Agreement question template.}
\begin{quote}
\ttfamily\scriptsize
Please indicate your level of agreement with the following statement: <STATEMENT>. Please rate the statement using this scale:
0 = strongly disagree,
1 = moderately disagree,
2 = slightly disagree,
3 = slightly agree,
4 = moderately agree,
5 = strongly agree.
\end{quote}

\textbf{Leading integer response instruction.}
\begin{quote}
\ttfamily\scriptsize
Your response should start with an integer from 0 to 5, followed by your reasoning.
\end{quote}

The three bracketed words iterated respectively over: persona text descriptions (see Appendix~\ref{app:personas}); the 15 relevance MFQ questions, and the 15 agreement MFQ questions.

\section{Moral Foundation Tables}
\label{app:moral-foundations-tables}
This appendix provides the numerical MFQ foundation profiles that correspond to Figures~\ref{fig:mfq-profiles} and~\ref{fig:persona-mfq-profiles}. Table~\ref{tab:moral_foundations_profiles} reports the self-assessment (no-persona) scores for each model, while Table~\ref{tab:persona_moral_foundations_profiles} lists the average scores for the persona sample discussed in the main text. Each entry is the mean rating with its associated standard error.

\begin{table}[t]
  \centering
  \caption{MFQ foundation profiles for no-persona self assessments. Values are mean ratings with standard errors computed across repeated questionnaire runs.}
  \label{tab:moral_foundations_profiles}
  \resizebox{\linewidth}{!}{\input{table_moral_foundations_profiles.tex}}
\end{table}

\begin{table}[t]
  \centering
  \caption{MFQ foundation profiles for sampled personas, averaged across models. Values are mean ratings with standard errors computed over models and repeated questionnaire runs.}
  \label{tab:persona_moral_foundations_profiles}
  \resizebox{\linewidth}{!}{\input{table_persona_moral_foundations_profiles.tex}}
\end{table}

It is intereting to compare the One can see that the only foundation for which models performed better than the average persona was Farirness/Reciprocity with the average over models being $4.36\pm 0.07$ and the average over personas being $4.25\pm0.01$. In contrast, the one for which the average over personas has the largest relative difference is Purity/Sanctity, $1.95\pm0.11$ vs $2.40\pm0.02$.



\subsection{Correlation Metric}
\label{sec:correlation_metrics}

We quantify how moral robustness and susceptibility co-vary by measuring the Pearson correlation coefficient between the two quantities across models. The coefficient is
\begin{equation}
  r_{RS} = \frac{\sum_i (R_i - \bar{R})(S_i - \bar{S})}{\sqrt{\sum_i (R_i - \bar{R})^2}\sqrt{\sum_i (S_i - \bar{S})^2}},
  \label{eq:pearson}
\end{equation}
where $R_i$ and $S_i$ denote the robustness and susceptibility of model $i$, and $\bar{R}$ and $\bar{S}$ are their respective means over all models. To propagate uncertainty we draw Gaussian samples $(R_i', S_i')$ using the standard errors for each model, recompute $r_{RS}$ for every draw, and quote the sample standard deviation of the resulting distribution. The same sampling procedure yields a family-level coefficient $\overline{r}_{RS}$ by first averaging $(R_i', S_i')$ within each model family before correlating. We repeat this computation for each moral foundation by restricting the robustness and susceptibility to the corresponding foundation-specific metrics.



\section{Parsing Failures}
\label{sec:failures}

In the first run, we constrain outputs to begin with a single integer rating from 0 to 5, and parse this leading integer. Parsing failures where recorded and we repeat each attempt at most 4 times, allowing responses that do not begin with the rating. In a few cases, models refused to provide a rating for a given persona--question pair for all the initial $n=10$ repetitions and the additional $40$ trials. Whenever this happened we excluded these personas from our analysis, because we need a matrix with all valid entries to compute the susceptibility, Eq.~\eqref{eq:overall-susceptibility}.

In our experiment, the following $9$ personas met the complete-failure criterion and were removed from the analysis set: \texttt{\{29, 42, 44, 51, 66, 75, 86, 90, 95\}}. We then chose the following grouping $|\mathcal{P}|-9=91= G\times |\mathcal{P}_G|=7 \times 13$ for estimating the moral susceptibility and its uncertainty.

Table~\ref{tab:failures_by_model} reports, for completeness, the total number of failed parsing rows and failed parsing attempts per model. The difference between the two columns gives a sense of the number of repetitions attempted. We list only models with non-zero totals.

\input{failures_by_model.tex}

Some model's responses systematically ignore the leading integer prompt instruction (see Appendix~\ref{app:prompts} for prompt details). In most cases they open with text such as ``As a~\ldots'' before eventually providing a rating. Most cases were model--question specific. However, some personas appeared repeatedly across models, and Table~\ref{tab:uninstructed-personas} highlights the two worst ``offenders" by aggregate parsing failures. This behavior was unexpected as their descriptions (see Appendix~\ref{app:personas}) do not obviously correlate with not following instructions, yet the pattern persists across architectures.

\begin{table}[t]
  \centering
  \caption{Personas with the highest parsing failure counts.}
  \label{tab:uninstructed-personas}
  \begin{tabular}{c|ccc|c}
    \toprule
    Persona ID & \texttt{gemini-2.5-flash-lite} & \texttt{gpt-4o} & \texttt{gpt-4o-mini} & Total failures \\
    \midrule
    66 & 30 & 6 & 60 & 96 \\
    94 & 58 & 4 & 30 & 92 \\
    \bottomrule
  \end{tabular}
\end{table}




\section{Personas}
\label{app:personas}
We evaluated models across a diverse set of personas, denoted as $\mathcal{P}$, to investigate how persona characteristics influence responses on the MFQ. We sampled $|\mathcal{P}| = 100$ personas from prior work on large-scale persona generation \citep{ge2025scalingsyntheticdatacreation}. Each persona description is enumerated below, with the enumeration linking each description to its corresponding persona ID.

\IfFileExists{appendix_personas.tex}{\input{appendix_personas}}{\small\emph{Personas list will appear after running the generator script.}}

\end{document}
