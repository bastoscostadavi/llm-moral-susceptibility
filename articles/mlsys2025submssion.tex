\documentclass{article}

% Allow builds from the repo root by searching both the current dir and articles/
\makeatletter
\def\input@path{{}{articles/}}
\makeatother

\usepackage{mlsys2025}
\usepackage[skip=6pt plus2pt, font=small]{caption}
\setlength{\textfloatsep}{10pt plus2pt minus2pt} 
\usepackage{natbib}
\usepackage{url}
\usepackage{graphicx}
\graphicspath{{../results/}{results/}}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{placeins}
\usepackage{listings}
\usepackage{xurl} % better URL line-breaking
\usepackage[hidelinks]{hyperref} % load last

% Listings setup for prompt blocks (two-column friendly, no boxes)
\lstset{
  basicstyle=\ttfamily\scriptsize,
  breaklines=true,
  breakatwhitespace=false, % allow breaking long tokens
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  tabsize=2,
  frame=none,
  xleftmargin=0pt,
  xrightmargin=0pt,
  aboveskip=0.5em,
  belowskip=0.5em,
}


\title{Moral Susceptibility and Robustness under Persona Role-Play \\ in Large Language Models}




% For double‑blind submission, leave author info empty under the MLSys style.
\author{}

\begin{document}

\maketitle

\begin{abstract}
  Large language models (LLMs) increasingly operate in social contexts, motivating analysis of how they form, maintain, and shift moral judgments. In this work, we investigate how persona role-play---prompting an LLM to assume a specific character---affects its moral profile. Using the Moral Foundations Questionnaire (MFQ), we introduce a benchmark that quantifies two properties: (i) moral susceptibility, their sensitivity to persona changes, and (ii) moral robustness, the consistency of model judgments under repeated sampling. For moral robustness, model family explains most of the variance, and model size shows no systematic effect. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible. We also observe an inverse correlation between robustness and susceptibility among families, with more robust models tending to be less susceptible. Additionally, we present moral foundation profiles for models without persona role-play and for averaged persona characterizations. Together, these analyses provide a systematic view of how persona conditioning shapes moral reasoning in LLMs.
\end{abstract}


\section{Introduction}
As large language models (LLMs) move into interactive, multi-agent settings, reliable benchmarks for their social reasoning are essential. Recent evaluations probe theory-of-mind, multi-agent interactions under asymmetric information, cooperation, and deception through controlled role-play and game-theoretic tasks \cite{zhou2024sotopia,pan2023machiavelli,bianchi2024negotiationarena,chen-etal-2024-tombench,costa2025deceivedetectdiscloselarge}. Complementary datasets benchmark social commonsense, moral judgment, and self-recognition capabilities \cite{sap-etal-2019-social,hendrycks2021ethics,bai2025knowthyselfincapabilityimplications}. Motivated by this landscape, we focus on moral judgment as a core facet of social decision-making and alignment.

This paper introduces a benchmark that combines persona role-play---prompting a LLM to assume a specific character---with the Moral Foundations Questionnaire \citep{moralfoundations2017questionnaires}, a widely used instrument in moral psychology that measures five moral foundations: Harm/Care, Fairness/Reciprocity, In-group/Loyalty, Authority/Respect, and Purity/Sanctity \citep{graham2009liberals,haidt2007when,moralfoundations2017questionnaires}. We elicit LLMs to respond to the MFQ while role-playing personas drawn from \citet{ge2025scalingsyntheticdatacreation}. From these responses, we define two complementary quantities: moral robustness, the stability of MFQ scores over personas under repeated sampling, and moral susceptibility, the sensitivity of MFQ scores to persona variation. These metrics are defined in Eq.~\eqref{eq:robustness} and Eq.~\eqref{eq:overall-susceptibility}, each with foundation-level decompositions and uncertainty estimates.

Applying this framework across contemporary model families and sizes, we find that model family accounts for most of the variance in moral robustness, with no systematic effect of model size. In contrast, moral susceptibility shows a mild family effect but a clear within-family size trend, with larger variants being more susceptible. Among individual models, Claude 4.5 Sonnet is the most robust and Grok 4 Fast the least. Conversely, Grok 4 Fast is the most susceptible, while GPT-4o Mini is the least. Overall, we qualitatively observe an inverse correlation between robustness and susceptibility among families, suggesting that models with more stable moral profiles tend to be less influenced by persona changes.

Recent research has examined the moral and social behavior of LLMs through the lens of the MFQ, exploring their value orientations, cultural variability, and alignment with human moral judgments \citep{abdulhai-etal-2024-moral,nunes2024hypocrites,aksoy2024whose,bajpai2024insights,ji2025moralbenchmoralevaluationllms}. Parallel efforts study persona role-playing as a mechanism for conditioning model behavior, including benchmarks, interactive environments, and diagnostic analyses \citep{tseng2024two,wang2023rolellm,samuel2025personagym,yu2025rpgbench,xu2024character,elboudouri2025rpeval,bai2025concept}. Our MFQ persona framework bridges these directions by systematically quantifying how persona conditioning alters moral judgments, separating the effects of repeated sampling (moral robustness) from those of persona variation (moral susceptibility). In addition, we report MFQ profiles for both unconditioned and persona-conditioned settings, providing a comparative view of baseline moral tendencies and persona-driven moral shifts across models.

\section{Moral Robustness and Susceptibility Benchmark}

We define a benchmark to evaluate the moral robustness and moral susceptibility of LLMs. Moral robustness is the stability of MFQ ratings across personas under repeated sampling, and moral susceptibility is the sensitivity of MFQ scores under different personas. These quantities are defined in Eq.~\eqref{eq:robustness} and Eq.~\eqref{eq:overall-susceptibility} respectively.

\subsection{Moral Foundation Questionnaire}

The Moral Foundations Questionnaire \citep{moralfoundations2017questionnaires} is a widely used instrument in moral psychology \citep{graham2009liberals,haidt2007when,moralfoundations2017questionnaires} and comprises 30 questions split into two sections. The first includes 15 relevance judgments, which assess how relevant certain considerations are when deciding what is right or wrong, and the second includes 15 agreement statements, which measure the level of agreement with specific moral propositions \citep{graham2011mfq,moralfoundations2017questionnaires}. In both sections, respondents answer each item using an integer scale from 0 to 5, representing in the first section the perceived relevance of the consideration and in the second the degree of agreement with the statement (see Appendix~\ref{app:prompts} for a verbatim description including the interpretation of the scale). Questions map to five moral foundations: Harm/Care, Fairness/Reciprocity, In-group/Loyalty, Authority/Respect, Purity/Sanctity. The results are typically presented as foundation-level scores, obtained by averaging the ratings of the questions associated with each foundation.

Figure~\ref{fig:mfq-profiles} illustrates the resulting foundation-level MFQ scores across models using no-persona role-play. Specifically, models were elicited to answer the 30 MFQ questions 10 times each, which we average by foundation and display with the corresponding standard error. Although not the focus of our work, understanding the moral profile of different frontier models is relevant, providing useful context for deployment and comparison.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../results/moral_foundations_relevance_profiles.png}
  \caption{Moral foundation profile across models with no-persona role-play (self). Points show mean rating per foundation; error bars denote standard errors across questions within each foundation.}
  \label{fig:mfq-profiles}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../results/persona_moral_foundations_relevance_profiles.png}
  \caption{Moral foundation profiles for ten randomly selected personas, averaged across models. See the Appendix~\ref{app:personas} for the mapping between persona IDs and their corresponding descriptions.}
  \label{fig:persona-mfq-profiles}
\end{figure}

Figure~\ref{fig:persona-mfq-profiles} illustrates the resulting foundation-level MFQ scores average over all models for different personas. It gives an average characterization of the moral persona role-play on models. The full per-persona, per-model and per-question MFQ ratings will be made publicly available after the review period.

\subsection{Experimental Methodology}
For each model, we iterate through all MFQ questions for every persona, repeating each question multiple times. Concretely we have:

\begin{itemize}
  \item \textbf{Personas:} We evaluate $|\mathcal{P}|=100$ persona descriptions drawn from prior work \citep{ge2025scalingsyntheticdatacreation}. Full persona descriptions and the corresponding ID–description mappings are provided in Appendix~\ref{app:personas}.
  \item \textbf{Prompting:} For each persona and question, the model receives a role-playing instruction: ``You are roleplaying as the following persona:", followed by the persona description text and one of the $|\mathcal{Q}|=30$ MFQ questions.\footnote{We query one MFQ question at a time rather than the full questionnaire in a single prompt to avoid sequence- and order-dependent effects. Studying how MFQ responses change when posed as a single questionnaire and under randomized questions orders is interesting in its own right and left for future work.} We instruct the models to start their response with the rating (an integer from 0 to 5), followed by their reasoning. Exact prompt templates are provided in Appendix~\ref{app:prompts}.
  \item \textbf{Repetition:} Each persona--question pair is queried \(n=10\) times to estimate within-persona mean score and variance, which are then used to compute the moral robustness and susceptibility, defined in Eq.~\eqref{eq:robustness} and Eq.~\eqref{eq:overall-susceptibility}. See Section~\ref{sec:rating_estimation} for a discussion of the underlying problem and an outline of a more principled approach.
  \item \textbf{Decoding:} In the first run, we constrain outputs to begin with a single integer rating from 0 to 5, and parse this leading integer. Parsing failures are recorded and we repeat each attempt at most 4 times, allowing responses that do not begin with the rating (see Section~\ref{sec:failures} for more details). This approach minimizes costs and unexpectedly revealed that some personas more likely elicit models to not follow instructions (see Section~\ref{sec:uninstructed_personas}).
  \item \textbf{Models:} We included: Claude Haiku 4.5, Claude Sonnet 4.5, Gemini 2.5 Flash Lite, GPT-4.1, GPT-4.1 Mini, GPT-4.1 Nano, GPT-4o, GPT-4o Mini, Grok 4 and Grok 4 Fast.
  \item \textbf{Logging}: For each model we did a total of $|\mathcal{Q}|\times|\mathcal{P}|\times n =30\times 100\times 10=30,000$ requests. The resulting tables will be made publicly available after the review period.
\end{itemize}
We next formalize how these repeated ratings are aggregated into moral robustness and susceptibility scores.

\subsection{Statistical Analysis}



This section formalizes the quantities we compute from the MFQ runs and how we summarize them into moral robustness and susceptibility metrics.

Let \(\mathcal{P}\) be the set of personas, \(\mathcal{Q}\) the set of 30 scored MFQ questions, and \(n\) the number of repeated queries per persona--question pair. For persona \(p\), question \(q\), and repetition \(i=1,\ldots,n\), let \(y_{pqi}\in\{0,\ldots,5\}\) be the parsed rating.

For each persona--question pair we compute the sample mean and the standard deviation across repetitions
\begin{align}
  \bar{y}_{pq} &= \frac{1}{n} \sum_{i=1}^{n} y_{pqi}, \label{eq:persona-question-mean}\\
  u_{pq} &= \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} \big(y_{pqi} - \bar{y}_{pq}\big)^2}, \label{eq:persona-question-se}
\end{align}

\paragraph{Moral robustness} We summarize within-pair variability by averaging the standard deviations in Eq.~\eqref{eq:persona-question-se} over personas and questions
\begin{equation}
  \bar{u} = \frac{1}{|\mathcal{P}|\,|\mathcal{Q}|} \sum_{p \in \mathcal{P}} \sum_{q \in \mathcal{Q}} u_{pq}.\label{eq:mean-uncertainty}
\end{equation}
Our robustness index is the reciprocal
\begin{equation}
  R = \frac{1}{\bar{u}}.\label{eq:robustness}
\end{equation}
Let the (sample) standard deviation of the \(u_{pq}\) values be
\begin{equation}
  s_u = \sqrt{\frac{1}{|\mathcal{P}|\,|\mathcal{Q}| - 1} \sum_{p \in \mathcal{P}} \sum_{q \in \mathcal{Q}} (u_{pq} - \bar{u})^2}.\label{eq:uncertainty-sd}
\end{equation}
Then the standard error of \(\bar{u}\) is \(\sigma_{\bar{u}} = s_u / \sqrt{|\mathcal{P}|\,|\mathcal{Q}|}\) which we propagate to get an estimate for the robustness standard error:
\begin{equation}
  \sigma_R = \frac{\sigma_{\bar{u}}}{\bar{u}^2}.
  \label{eq:robustness-se}
\end{equation}
Foundation-specific robustness reuse Eqs.~\eqref{eq:mean-uncertainty}--\eqref{eq:robustness-se} after restricting \(\mathcal{Q}\) to the question subset \(\mathcal{Q}_f\) for foundation \(f\). Having defined the within-persona variability, we now turn to between-persona dispersion.

\paragraph{Moral susceptibility} To stabilize estimates across many personas, we partition \(\mathcal{P}\) into \(G\) disjoint groups \(\mathcal{P}_1,\ldots,\mathcal{P}_G\) of equal size. For each question \(q\) and group \(g\), we compute the sample standard deviation of persona means
\begin{align}
  & s_{qg} = \sqrt{\frac{1}{|\mathcal{P}_g|-1} \sum_{p \in \mathcal{P}_g}(\bar{y}_{pq} - \bar{y}_{gq})^2},
  \label{eq:question-dispersion}
\end{align}
with $\bar{y}_{gq}$ the average over $\mathcal{P}_g$, i.e.: 
\begin{align}
  \bar{y}_{gq} = \frac{1}{|\mathcal{P}_g|} \sum_{p \in \mathcal{P}_g} \bar{y}_{pq}.
\end{align}
From $s_{qg}$ we obtain a group-level susceptibility sample
\begin{equation}
  S_g = \frac{1}{|\mathcal{Q}|} \sum_{q \in \mathcal{Q}} s_{qg}.\label{eq:group-susceptibility}
\end{equation}
The reported susceptibility is the mean over groups
\begin{equation}
  S = \frac{1}{G} \sum_{g=1}^{G} S_g,\label{eq:overall-susceptibility}
\end{equation}
with its standard error estimated from the between-group variability
\begin{equation}
  \sigma_S = \frac{1}{\sqrt{G}}\sqrt{\frac{1}{G-1} \sum_{g=1}^{G} (S_g - S)^2}.\label{eq:susceptibility-se}
\end{equation}
Foundation-specific susceptibilities reuse Eqs.~\eqref{eq:question-dispersion}--\eqref{eq:susceptibility-se} after restricting \(\mathcal{Q}\) to the question subset \(\mathcal{Q}_f\) for foundation \(f\).

\paragraph{Cross-model normalization} To facilitate comparison, we also present the $z$-scores that summarize relative performance across models. The $z$-score for moral metric $M\in \{S,R\}$ is
\begin{equation}
  z_{M} = \frac{M - \mu_M}{\sigma_M},
  \label{eq:zscore}
\end{equation}
where $M$ is the models's score, $\mu_M$ is the mean, and $\sigma_M$ is the standard deviation over different models. The uncertainty of $z_M$ is propagated from that of $M$, $\mu_M$ and $\sigma_M$.


\subsection{Average Score and Variance Estimation}
\label{sec:rating_estimation}

The first step to get the moral robustness and susceptibility is to compute the sample mean score and variance, Eq.~\eqref{eq:persona-question-mean} and Eq.~\eqref{eq:persona-question-se}. Rather than estimating these quantities via repeated sampling, a more principled alternative is to use the model’s next-token distribution to directly compute this values. Given the question prompt (that includes a the instruction that the response should begin with the rating from 0--5), let \(p_n = p(n\mid\text{prompt})\) denote the probability that the next token is the digit \(n\). Then, the average score and variance are given exactly by:
\begin{align}
  \mathbb{E}[n] = \sum_{n=0}^5 np_n, \quad \operatorname{Var}(n) = \sum_{n=0}^5 (n-\mathbb{E}[n])^2p_n
\end{align}
This is the average and variance that our 10-trial procedure approximates, while avoiding parsing failures. Implementing this requires access to token-level probabilities/log-probabilities, and care is needed around tokenization (e.g., space-prefixed digits or multiple token aliases).


\subsection{Failures to Respond}
\label{sec:failures}

In the first run, we constrain outputs to begin with a single integer rating from 0 to 5, and parse this leading integer. Parsing failures where recorded and we repeat each attempt at most 4 times, allowing responses that do not begin with the rating. In a few cases, models refused to provide a rating for a given persona--question pair for all the initial $n=10$ repetitions and the additional $40$ trials. Whenever this happened we excluded these personas from our analysis, because we need a matrix with all valid entries to compute the susceptibility, Eq.~\eqref{eq:overall-susceptibility}, and its uncertainty, Eq.~\eqref{eq:susceptibility-se}.

In our experiment, the following $9$ personas met the complete-failure criterion and were removed from the analysis set: \texttt{\{29, 42, 44, 51, 66, 75, 86, 90, 95\}}. We then chose the following grouping $|\mathcal{P}|-9=91= G\times |\mathcal{P}_G|=7 \times 13$ for estimating the moral susceptibility and its uncertainty.

Table~\ref{tab:failures_by_model} reports, for completeness, the total number of failed parsing rows and failed parsing attempts per model. The difference between the two columns gives a sense of the number of repetitions attempted. We list only models with non-zero totals. In the table, items with ``(self)" indicate the batch with no persona role-play.

\begin{table}[t]
  \centering
  \caption{Total parsing failure counts per model.}
  \label{tab:failures_by_model}
  \begin{tabular}{lcc}
    \toprule
    Model & Failed rows & Total failures \\
    \midrule
    claude-sonnet-4-5 & 24 & 37  \\
    claude-sonnet-4-5 (self) & 213	& 213  \\
    gemini-2.5-flash-lite & 129 & 344 \\
    gemini-2.5-flash-lite (self) & 6 & 6 \\
    gpt-4.1 & 4 & 4   \\
    gpt-4.1 (self) & 13 & 51   \\
    gpt-4o & 24 & 37  \\
    gpt-4o (self) & 19 & 41  \\
    gpt-4o-mini & 71 & 202 \\
    gpt-4o-mini (self) & 18 & 38 \\
    grok-4 (self) & 5 & 5 \\
    \bottomrule
  \end{tabular}
\end{table}




\section{Results}

Our results for the overall moral robustness, Eq.~\eqref{eq:robustness}, and susceptibility, Eq.~\eqref{eq:overall-susceptibility}, by model are displayed in Figure~\ref{fig:overall}. To facilitate comparison we also present the $z$-scores, Eq.~\eqref{eq:zscore}, in Table~\ref{tab:summary_by_model_with_z}.  We observe a qualitative inverse correlation between moral robustness and susceptibility among families, with the Grok family the most susceptible and least robust, and the Claude family the most robust and one of the least susceptible.

% Overall Susceptibility and Robustness
\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.9\linewidth]{../results/moral_metrics_overall_bars.pdf}\hfill
  \caption{Overall susceptibility and robustness across models. Error bars show propagated standard error via delta method; higher susceptibility values indicate larger persona-driven shifts in MFQ subscale scores; higher robustness values indicate greater rating stability.}
  \label{fig:overall}
\end{figure*}

\input{table_summary_by_model_with_z.tex}

\subsection{Moral Robustness}

Our results for foundation-level moral robustness Eq.~\eqref{eq:robustness} is displayed in Figure~\ref{fig:robustness}. Moral robustness exhibits clear within-family structure across models. The Claude family is consistently the most robust, outperforming all other models by a sizeable margin across all foundations. In contrast, the Grok models are the least robust, underperforming all other models by a sizeable margin across all foundations. On the other hand, model size does not appear to have a systematic effect on moral robustness. These trends are visible in Figure~\ref{fig:robustness} and summarized in the $z$-score Table~\ref{tab:summary_by_model_with_z}.

% Robustness (overall + five foundations)
\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.9\linewidth]{../results/robustness_bars.pdf}\hfill
  \caption{Moral robustness foundation profile across models, Eq.~\eqref{eq:robustness}. Error bars show propagated standard error, Eq.~\eqref{eq:robustness-se}; higher values indicate greater rating stability. The highlighted bars indicate the overall robustness over all foundations for that model. Note that the x-axis range varies across models, see Figure~\ref{fig:overall} to visualize the different scales.}
  \label{fig:robustness}
\end{figure*}

\subsection{Moral Susceptibility}

Our results for foundation-level moral susceptibility Eq.~\ref{eq:overall-susceptibility} are displayed in Figure~\ref{fig:susceptibility}. Moral susceptibility exhibits a mild family effect as families tend to lie close together. However, there is a clear within-family size effect with larger variants having higher moral susceptibility. We refrain from fitting parametric trends versus model size because most model sizes are not publicly disclosed. These patterns are visible in Figure~\ref{fig:susceptibility} and summarized in the $z$-score Table~\ref{tab:summary_by_model_with_z}. The most susceptible model overall is Grok-4-fast and the least is GPT-4o Mini.

% Susceptibility (overall + five foundations)
\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.9\linewidth]{../results/susceptibility_bars.pdf}\hfill
  \caption{Moral susceptibility foundation profile across models, Eq.~\eqref{eq:overall-susceptibility}. Error bars show propagated standard error, Eq.~\eqref{eq:susceptibility-se}; higher values indicate larger persona-driven shifts in MFQ scores. The highlighted bars indicate the overall susceptibility over all foundations for that model. Note that the x-axis range varies across models, see Figure~\ref{fig:overall} to visualize the different scales.}
  \label{fig:susceptibility}
\end{figure*}


\subsection{Uninstructed Personas}
\label{sec:uninstructed_personas}

Some model's responses systematically ignore the leading integer prompt instruction (see Appendix~\ref{app:prompts} for prompt details). In most cases they open with text such as ``As a~\ldots'' before eventually providing a rating. Most cases were model--question specific. However, some personas appeared repeatedly across models, and Table~\ref{tab:uninstructed-personas} highlights the two worst ``offenders" by aggregate parsing failures. This behavior was unexpected as their descriptions (see Appendix~\ref{app:personas}) do not obviously correlate with not following instructions, yet the pattern persists across architectures.

\begin{table}[t]
  \centering
  \caption{Personas with the highest parsing failures counts.}
  \label{tab:uninstructed-personas}
  \begin{tabular}{lrr}
    \toprule
    Persona ID & 66 & 94 \\
    \midrule
    \texttt{gemini-2.5-flash-lite} & 30.0 & 58.0 \\
    \texttt{gpt-4o} & 6.0 & 4.0\\
    \texttt{gpt-4o-mini} & 60.0 & 30.0\\
    \midrule
    Total failures & 96.0 & 92.0\\
    \bottomrule
  \end{tabular}
\end{table}


\section{Conclusion}
We present a benchmark for evaluating how persona role-play shapes moral reasoning in large language models using the Moral Foundations Questionnaire. By distinguishing moral robustness (stability across samples) from moral susceptibility (sensitivity to persona variation), our results reveal consistent family-level patterns and a size-dependent susceptibility trend. Together, these results offer a systematic framework for comparing moral profiles across model families and sizes, providing a quantitative basis for future studies of moral behavior in language models.



% Use the official MLSys 2025 bibliography style when available
\bibliographystyle{mlsys2025}
\bibliography{references}


\appendix

\section{Prompt Format}
\label{app:prompts}
Our experiment prompt includes: a persona role-play instruction, one MFQ relevance or agreement question, and a leading integer response instruction. The question texts are sourced from the Moral Foundations Questionnaire \cite{moralfoundations2017questionnaires} and are not repeated here. Below we show the exact prompt formats used.

\begin{itemize}
\item \textbf{Persona role-play instruction}
\begin{lstlisting}
You are roleplaying as the following persona: 
<DESCRIPTION>

Please answer the following question.
\end{lstlisting}

\item \textbf{Relevance question template}

\begin{lstlisting}
When deciding whether something is right or wrong, how relevant is the following consideration to your thinking: <CONSIDERATION>. Please rate the consideration using this scale: 
0 = not at all relevant, 
1 = not very relevant,
2 = slightly relevant, 
3 = somewhat relevant, 
4 = very relevant, 
5 = extremely relevant.
\end{lstlisting}
\item \textbf{Agreement question template}
\begin{lstlisting}
Please indicate your level of agreement with the following statement: <STATEMENT>. Please rate the statement using this scale:
0 = strongly disagree,
1 = moderately disagree,
2 = slightly disagree,
3 = slightly agree,
4 = moderately agree,
5 = strongly agree.
\end{lstlisting}
\item \textbf{Leading integer response instruction}
\begin{lstlisting}
Your response should start with an integer from 0 to 5, followed by your reasoning.
\end{lstlisting}
\end{itemize}

The three bracketed words iterated respectively over: persona text descriptions (see Appendix~\ref{app:personas}); the 15 relevance MFQ questions, and the 15 agreement MFQ questions.



\section{Personas}
\label{app:personas}
We evaluated models across a diverse set of personas, denoted as $\mathcal{P}$, to investigate how persona characteristics influence responses on the MFQ. We sampled $|\mathcal{P}| = 100$ personas from prior work on large-scale persona generation \citep{ge2025scalingsyntheticdatacreation}. Each persona description is enumerated below, with the enumeration linking each description to its corresponding persona ID.

\IfFileExists{appendix_personas.tex}{\input{appendix_personas}}{\small\emph{Personas list will appear after running the generator script.}}

\end{document}
